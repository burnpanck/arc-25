{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ee69f0d-7d82-4e20-af16-ec5052013a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import lzma\n",
    "import os\n",
    "import dataclasses\n",
    "import itertools\n",
    "import functools\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "from types import MappingProxyType, SimpleNamespace\n",
    "\n",
    "import cbor2\n",
    "import attrs\n",
    "import tqdm.auto\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "import optax\n",
    "import jaxtyping as jt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from arc25 import symmetry, tools as arc25_tools\n",
    "from arc25.symmetry import D4, transform_vector\n",
    "from arc25 import serialisation\n",
    "from arc25.dsl.types import Vector, Dir4\n",
    "from arc25.vision2.symrep import SymDecompBase, SplitSymDecomp, SymDecompDims, standard_rep, RepSpec\n",
    "from arc25.vision2.fields import FieldDims, CoordinateGrid\n",
    "from arc25.vision2.linear import SpaceSymmetricLinear, SpaceSymmetricTensor, SymmetryMappingSpec, SymDecompLinear\n",
    "from arc25.vision2 import fields, attention, encoder, transformer, mae, swiglu, arc_solver\n",
    "from arc25.training import saving, dataset, mae as mae_trainer, knn_eval, linear_probe, arc_solver as solver_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfb29360-f3d7-4c0c-ba88-5665a46f757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"XLA_FLAGS\"]=\"--xla_force_host_platform_device_count=2\"\n",
    "os.environ[\"EPATH_USE_TF\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37284a09-443f-485a-a794-b50d8b83fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_root = Path(\"..\").resolve()\n",
    "data_root = proj_root / \"data\"\n",
    "model_dir = data_root / \"models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bbf64f7-671a-40cd-a2e4-a0db869710ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518152\n"
     ]
    }
   ],
   "source": [
    "src_dataset = dataset.ImagesDataset.load_compressed_cbor(\n",
    "    data_root/\"repack/re-arc.cbor.xz\",\n",
    "    filter=lambda iop,ex:  iop.input.shape == iop.output.shape,\n",
    ")\n",
    "challenge_order = tuple(sorted(src_dataset.challenges))\n",
    "print(len(src_dataset.examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4ef52b-7baf-4de0-8898-1cc3dcb7fd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring 3 unpaired output images\n",
      "training_ds: {(30, 30): 35519, (21, 30): 69584, (21, 21): 39576, (12, 12): 21717, (12, 21): 48056, (12, 30): 43704}\n"
     ]
    }
   ],
   "source": [
    "reload(dataset)\n",
    "\n",
    "small_eval = True\n",
    "\n",
    "eval_split, train_split = src_dataset.split_by_challenge(\n",
    "    np.random.default_rng(seed=42),\n",
    "    n_min=6 if small_eval else 100,\n",
    ")\n",
    "\n",
    "input_ds, output_ds = train_split.split_input_output()\n",
    "\n",
    "size_cuts = [12, 21, 30]\n",
    "#size_cuts = [8,12,16,24,30]\n",
    "#size_cuts = [30]\n",
    "\n",
    "training_ds, = [dataset.BucketedDataset.make(\n",
    "    s,\n",
    "    set(itertools.product(size_cuts, size_cuts)) if s is not {} else [(30,30)],\n",
    "    challenges=challenge_order,\n",
    ") for s in [output_ds]]\n",
    "\n",
    "for k,v in dict(\n",
    "    training_ds=training_ds,\n",
    ").items():\n",
    "    print(f\"{k}: { {kk:vv.n_examples for kk,vv in v.buckets.items()} }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e4876b2-2015-43f1-b6f4-7d58e96204b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(solver_trainer)\n",
    "\n",
    "config = solver_trainer.ArcSolverConfig(\n",
    "    seed = 42,\n",
    "    \n",
    "    batch_size = 16,\n",
    "    ref_batch = 16,\n",
    "    minibatch_size = 16,\n",
    "    base_cell_cost = 10, \n",
    "        \n",
    "    learning_rate = 1e-5,\n",
    "    max_num_epochs = 1,\n",
    "    max_num_ref_batches = 128,\n",
    "\n",
    "    warmup_steps = 64,\n",
    "    checkpoint_every_steps = 16,\n",
    "    \n",
    "    mode=\"flat\",\n",
    "    remat=True,\n",
    "    unroll=None,\n",
    "\n",
    "    eval_every_ref_batch = 0.5,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53b2f01d-f728-47f2-983f-0a323564e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_devices = jax.local_device_count()\n",
    "\n",
    "minibatch_size_fn = dataset.MinibatchSizeFunction(\n",
    "    reference_minibatch_size=config.minibatch_size,\n",
    "    reference_image_size=config.reference_image_size,\n",
    "    base_cost=config.base_cell_cost,\n",
    "    granularity=num_devices,  # Ensure divisibility for pmap\n",
    ")\n",
    "\n",
    "batch_spec = dataset.BatchSpec(\n",
    "    target_batch_weight=config.batch_size,\n",
    "    reference_image_size=config.reference_image_size,\n",
    "    # each image gets equal weight\n",
    "    area_weight_exponent=None,\n",
    ")\n",
    "\n",
    "collator = dataset.BucketedCollator.make(\n",
    "    dataset=training_ds,\n",
    "    batch_spec=batch_spec,\n",
    "    minibatch_size=minibatch_size_fn,\n",
    "    seed=42, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52574434-e275-4a6b-b746-a11bae47fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_src = dataset.OnDemandBucketDataset(\n",
    "    input_ds,\n",
    "    bucket_shapes = tuple(sorted(training_ds.buckets.keys(),key=lambda sh:(sh[0]*sh[1],abs(sh[0]-sh[1])))),\n",
    "    challenges = challenge_order,\n",
    "    weight_fun = lambda area: None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f1e664c-8d6e-45ae-8467-87c2842b0e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(arc_solver)\n",
    "\n",
    "model_config = {\n",
    "    k:v\n",
    "    for k,v in mae.configs[\"tiny\"].items()\n",
    "    if k not in {\"decoder_cell_infusion\"}\n",
    "}\n",
    "model_config.update(\n",
    "    num_program_tokens = 4,\n",
    "    num_latent_programs = len(challenge_order),\n",
    ")\n",
    "\n",
    "solver = arc_solver.ARCSolver(\n",
    "    **model_config,\n",
    "    dtype=jnp.float32,\n",
    "    rngs=nnx.Rngs(42),\n",
    ")\n",
    "width = solver.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a18a3a8-afaf-4cda-b339-86cd5bbc6923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading encoder from checkpoint: /Users/yves/git-private/arc-2025/data/models/20251023-1137-vertex-ai-mae-tiny-4xL4-chkp-006912.msgpack.xz\n",
      "Encoder loaded successfully\n"
     ]
    }
   ],
   "source": [
    "chkp_path = model_dir / \"20251023-1137-vertex-ai-mae-tiny-4xL4-chkp-006912.msgpack.xz\"\n",
    "print(f\"Loading encoder from checkpoint: {chkp_path}\")\n",
    "encoder_checkpoint = saving.load_model(chkp_path)\n",
    "nnx.update(solver.encoder, encoder_checkpoint.state.model.encoder)\n",
    "print(\"Encoder loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1cca168-a2b6-4b12-a3a1-d85825147786",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = solver_trainer.ArcSolverTrainer.make(\n",
    "    config=config,\n",
    "    model=solver,\n",
    "    collator=collator,\n",
    "    inputs_src=input_src,\n",
    "    eval_dataset = eval_split,\n",
    "    num_devices = num_devices,\n",
    "    rngs = nnx.Rngs(42),\n",
    "    with_progress_bars = True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a43fa53a-8971-4524-bc44-90c3765b92a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ArcSolverTrainer ---\n",
      "Run: 20251026-1559-ArcSolverTrainer\n",
      "Devices: 2 Ã— cpu\n",
      "Training batch data weight: 16 (1 optimizer step)\n",
      "Reference step data weight: 16 (~1.00 optimizer steps)\n",
      "Total steps: 128\n",
      "Evaluation: every 0.5 reference steps\n",
      "----------------------------\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22c937abf8a4561b721d0b39a23802a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing _compute_grads for shape dict(cell_weight=(4,21,21), input_sizes=(4,2), inputs=(4,21,21), latent_program_idx=(4), loss_focus=(1), output_masks=(4,21,21), outputs=(4,21,21), reference_entropy=(1)) (kw=dict(mode='flat', remat=True, unroll=None))\n",
      "Tracing _compute_grads for shape dict(cell_weight=(5,12,30), input_sizes=(5,2), inputs=(5,12,30), latent_program_idx=(5), loss_focus=(1), output_masks=(5,12,30), outputs=(5,12,30), reference_entropy=(1)) (kw=dict(mode='flat', remat=True, unroll=None))\n",
      "Tracing _apply_update\n",
      "Tracing _compute_grads for shape dict(cell_weight=(5,12,30), input_sizes=(5,2), inputs=(5,12,30), latent_program_idx=(5), loss_focus=(1), output_masks=(5,12,30), outputs=(5,12,30), reference_entropy=(1)) (kw=dict(mode='flat', remat=True, unroll=None))\n",
      "Tracing _compute_grads for shape dict(cell_weight=(2,30,30), input_sizes=(2,2), inputs=(2,30,30), latent_program_idx=(2), loss_focus=(1), output_masks=(2,30,30), outputs=(2,30,30), reference_entropy=(1)) (kw=dict(mode='flat', remat=True, unroll=None))\n",
      "Tracing _apply_update\n",
      "\n",
      "[Step 1] Preparing input embeddings for evaluation...\n",
      "WARNING: Ignoring 3 unpaired input images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51e5a9f8a9242e29e136e2cb28c28fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing _embed_inputs for shape dict(input_sizes=(2,32,2), inputs=(2,32,12,12)) (kw=dict(mode='flat', remat=True, unroll=None, deterministic=True))\n",
      "Tracing _embed_inputs for shape dict(input_sizes=(2,79,2), inputs=(2,79,12,21)) (kw=dict(mode='flat', remat=True, unroll=None, deterministic=True))\n",
      "Tracing _embed_inputs for shape dict(input_sizes=(2,66,2), inputs=(2,66,12,30)) (kw=dict(mode='flat', remat=True, unroll=None, deterministic=True))\n",
      "Tracing _embed_inputs for shape dict(input_sizes=(2,60,2), inputs=(2,60,21,21)) (kw=dict(mode='flat', remat=True, unroll=None, deterministic=True))\n",
      "Tracing _embed_inputs for shape dict(input_sizes=(2,103,2), inputs=(2,103,21,30)) (kw=dict(mode='flat', remat=True, unroll=None, deterministic=True))\n",
      "Tracing _embed_inputs for shape dict(input_sizes=(2,53,2), inputs=(2,53,30,30)) (kw=dict(mode='flat', remat=True, unroll=None, deterministic=True))\n",
      "Embedding inputs for evaluation completed in 102.7s\n",
      "\n",
      "[Step 1] Running evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fa09ffc5f74ae189317fb9d0ecd076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing _evaluate for shape dict(cell_weight=(2,32,12,12), embeddings=(2, 32), latent_program_idx=(2,32), output_masks=(2,32,12,12), output_sizes=(2,32,2), outputs=(2,32,12,12)) (kw=dict(mode='flat', remat=True, unroll=None, deterministic=True))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All input arrays must have the same shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m stats = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-private/arc-2025/src/arc25/training/base.py:614\u001b[39m, in \u001b[36mTrainerBase.run_main\u001b[39m\u001b[34m(self, checkpoint_dir, wandb_project, run_name)\u001b[39m\n\u001b[32m    609\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m stats_dev, is_jit_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.train(\n\u001b[32m    610\u001b[39m     start_weight=resume_accumulated_weight\n\u001b[32m    611\u001b[39m ):\n\u001b[32m    612\u001b[39m     \u001b[38;5;66;03m# Process pending stats from previous step\u001b[39;00m\n\u001b[32m    613\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pending_stats \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m         \u001b[43mprocess_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpending_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpending_is_jit_step\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_jit_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    616\u001b[39m     \u001b[38;5;66;03m# Store current stats as pending\u001b[39;00m\n\u001b[32m    617\u001b[39m     pending_stats = stats_dev\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-private/arc-2025/src/arc25/training/base.py:554\u001b[39m, in \u001b[36mTrainerBase.run_main.<locals>.process_stats\u001b[39m\u001b[34m(stats_dev, was_jit_step)\u001b[39m\n\u001b[32m    549\u001b[39m \u001b[38;5;66;03m# Check if it's time for Periodic evaluation (task-specific)\u001b[39;00m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    551\u001b[39m     training_progress // config.eval_every_ref_batch\n\u001b[32m    552\u001b[39m     > prev_progress // config.eval_every_ref_batch\n\u001b[32m    553\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     eval_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mperiodic_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    556\u001b[39m     eval_result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-private/arc-2025/src/arc25/training/arc_solver.py:669\u001b[39m, in \u001b[36mArcSolverTrainer.periodic_evaluation\u001b[39m\u001b[34m(self, stats)\u001b[39m\n\u001b[32m    667\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[Step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Running evaluation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    668\u001b[39m eval_start = time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m eval_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    670\u001b[39m eval_results.pop(\u001b[33m\"\u001b[39m\u001b[33mper_class\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# not suitable for wandb\u001b[39;00m\n\u001b[32m    671\u001b[39m eval_time = time.monotonic() - eval_start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-private/arc-2025/src/arc25/training/arc_solver.py:608\u001b[39m, in \u001b[36mArcSolverTrainer._evaluate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    606\u001b[39m minibatch = jax.tree.map(\u001b[38;5;28;01mlambda\u001b[39;00m a: reshard(a, \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m), minibatch)\n\u001b[32m    607\u001b[39m params = jax.tree.map(\u001b[38;5;28;01mlambda\u001b[39;00m a: reshard(a), params)\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m stats = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresharded_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mminibatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m            \u001b[49m\u001b[43mremat\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m            \u001b[49m\u001b[43munroll\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43munroll\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    622\u001b[39m     res = stats\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.7/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/flax/nnx/transforms/compilation.py:433\u001b[39m, in \u001b[36mJitWrapped.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m graph.update_context(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    432\u001b[39m   pure_args, pure_kwargs = \u001b[38;5;28mself\u001b[39m._get_pure_args_kwargs(args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m   pure_args_out, pure_kwargs_out, pure_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjitted_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43mpure_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpure_kwargs\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    436\u001b[39m   out = \u001b[38;5;28mself\u001b[39m._get_non_pure_out(pure_args_out, pure_kwargs_out, pure_out)\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "    \u001b[31m[... skipping hidden 15 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.7/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/flax/nnx/transforms/compilation.py:128\u001b[39m, in \u001b[36mJitFn.__call__\u001b[39m\u001b[34m(self, *pure_args, **pure_kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *pure_args, **pure_kwargs):\n\u001b[32m    121\u001b[39m   args, kwargs = extract.from_tree(\n\u001b[32m    122\u001b[39m     (pure_args, pure_kwargs),\n\u001b[32m    123\u001b[39m     merge_fn=_jit_merge_fn,\n\u001b[32m    124\u001b[39m     ctxtag=\u001b[38;5;28mself\u001b[39m.ctxtag,\n\u001b[32m    125\u001b[39m     is_inner=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    126\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m   out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m   args_out, kwargs_out = extract.clear_non_graph_nodes((args, kwargs))\n\u001b[32m    131\u001b[39m   pure_args_out, pure_kwargs_out, pure_out = extract.to_tree(\n\u001b[32m    132\u001b[39m     (args_out, kwargs_out, out),\n\u001b[32m    133\u001b[39m     prefix=(\u001b[38;5;28mself\u001b[39m.in_shardings, \u001b[38;5;28mself\u001b[39m.kwarg_shardings, \u001b[38;5;28mself\u001b[39m.out_shardings),\n\u001b[32m    134\u001b[39m     ctxtag=\u001b[38;5;28mself\u001b[39m.ctxtag,\n\u001b[32m    135\u001b[39m     split_fn=_jit_split_fn,\n\u001b[32m    136\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-private/arc-2025/src/arc25/training/arc_solver.py:285\u001b[39m, in \u001b[36mTrainState._evaluate\u001b[39m\u001b[34m(model, minibatch_dict, params, kw)\u001b[39m\n\u001b[32m    270\u001b[39m pair_correct = (\n\u001b[32m    271\u001b[39m     (\n\u001b[32m    272\u001b[39m         \u001b[38;5;66;03m# Padding doesn't count against accuracy\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    277\u001b[39m     .astype(jnp.float32)\n\u001b[32m    278\u001b[39m )\n\u001b[32m    279\u001b[39m pair_accuracy = (\n\u001b[32m    280\u001b[39m     jnp.where(pair_correct, image_weights, \u001b[32m0\u001b[39m)\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m image_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m pair_correct\n\u001b[32m    283\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m per_image_stats = \u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpair_crossentropy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcell_accuracy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpair_accuracy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m K = model.latent_program_embeddings.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    297\u001b[39m per_class_stats = (\n\u001b[32m    298\u001b[39m     jnp.zeros((K, \u001b[32m5\u001b[39m), dtype=per_image_stats.dtype)\n\u001b[32m    299\u001b[39m     .at[latent_program_idx]\n\u001b[32m    300\u001b[39m     .add(per_image_stats)\n\u001b[32m    301\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.13.7/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/jax/_src/numpy/lax_numpy.py:4430\u001b[39m, in \u001b[36mstack\u001b[39m\u001b[34m(arrays, axis, out, dtype)\u001b[39m\n\u001b[32m   4428\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays:\n\u001b[32m   4429\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m np.shape(a) != shape0:\n\u001b[32m-> \u001b[39m\u001b[32m4430\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll input arrays must have the same shape.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4431\u001b[39m   new_arrays.append(expand_dims(a, axis))\n\u001b[32m   4432\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m concatenate(new_arrays, axis=axis, dtype=dtype)\n",
      "\u001b[31mValueError\u001b[39m: All input arrays must have the same shape."
     ]
    }
   ],
   "source": [
    "stats = trainer.run_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf19721-bbe8-4bb5-9075-5e5bb1c9e1a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cache = trainer._eval_data_cache\n",
    "print(cache.total_weight, cache.per_class_total_weight.sum())\n",
    "print(cache.per_class_total_weight)\n",
    "print(sum(w.sum() if (w:=mb.get(\"weight\")) is not None else mb[\"latent_program_idx\"].size for mb in cache.minibatches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62740b3-8c98-4e15-a237-5b51f411f15e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
