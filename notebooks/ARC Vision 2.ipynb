{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e92cc294-fc57-4ca6-a250-317eeeba70b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import lzma\n",
    "import os\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "from types import MappingProxyType, SimpleNamespace\n",
    "\n",
    "import cbor2\n",
    "import attrs\n",
    "import tqdm.auto\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "import optax\n",
    "import jaxtyping as jt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from arc25 import symmetry\n",
    "from arc25.symmetry import D4, transform_vector\n",
    "from arc25 import serialisation\n",
    "from arc25.dsl.types import Vector, Dir4\n",
    "from arc25.vision2.symrep import SymDecompBase, SplitSymDecomp, SymDecompDims, standard_rep, RepSpec\n",
    "from arc25.vision2.fields import FieldDims\n",
    "from arc25.vision2.linear import SpaceSymmetricLinear, SpaceSymmetricTensor, SymmetryMappingSpec, SymDecompLinear\n",
    "from arc25.vision2 import attention, encoder, transformer, classification, swiglu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62411ecc-c33b-4af9-b2b6-71b6674fd2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"XLA_FLAGS\"]=\"--xla_force_host_platform_device_count=2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b3311db-eae3-47d7-8f69-d33385ca5397",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_root = Path(\"..\").resolve()\n",
    "data_root = proj_root / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b090806-d291-4fa3-adfd-da0e9594a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# helper functions\n",
    "#----------------------\n",
    "def pytree_structure(pytree, title='pytree structure'):\n",
    "  print(f\"{title}:\")\n",
    "  for path, value in jax.tree.leaves_with_path(pytree):\n",
    "    print(f\"- pytree{jax.tree_util.keystr(path)} = {value.shape!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "917e421c-16fc-4fa0-964a-c112de2d1cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant(key, shape, dtype):\n",
    "    seed = np.array(jax.random.key_data(key)).astype(\"u4\")\n",
    "    seed = (seed[0] ^ seed[1])&0x7fff_ffff\n",
    "    rng = np.random.RandomState(seed)\n",
    "    return rng.randint(-3, 4, size=shape).astype(dtype) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34d49584-d458-47a7-a1c8-fe0161d7fef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(attention)\n",
    "reload(swiglu)\n",
    "reload(transformer)\n",
    "reload(encoder)\n",
    "\n",
    "# prepare an example attention module with prime dimensions\n",
    "width = FieldDims(\n",
    "    context = SymDecompDims(\n",
    "        space = 2*16,   # 8x2 = 16   \n",
    "        flavour = 1*16, # 10x1 = 10\n",
    "        invariant= 14*16, # 1x14 -> 40*16\n",
    "    ),\n",
    "    cells = SymDecompDims(\n",
    "        space = 2*8,\n",
    "        flavour = 1*8,\n",
    "        invariant = 22*8, # -> 48*8\n",
    "    ),\n",
    "    context_tokens = 0,\n",
    ")\n",
    "    \n",
    "enc = encoder.ARCEncoder(\n",
    "    num_heads=8,\n",
    "    num_groups=2,\n",
    "    num_layers=8,\n",
    "    hidden_size=width,\n",
    "    swiglu_width_factor=8/3,\n",
    "    qk_head_width=SymDecompDims(\n",
    "        space = 3 * 8,  # 1x3x8\n",
    "        flavour = 1 * 4,  # 10x1x4 = 5x1x8 \n",
    "        invariant = 4 * 8, # 1*4*8 -> 12x8\n",
    "        rep=RepSpec(symmetry.TrivialRep, 10)\n",
    "    ),\n",
    "    v_head_width=SymDecompDims(\n",
    "        space = 2*4,\n",
    "        flavour = 1*4,\n",
    "        invariant = 22*4,\n",
    "    ),\n",
    "    use_chirality_rep=False,\n",
    "#    kernel_init=quant,\n",
    "#    bias_init=quant,\n",
    "    per_head_rope_freq=False,\n",
    "    dtype=np.float32,\n",
    "#    activation=jax.nn.relu,\n",
    "#    use_bias=False,\n",
    "    rngs=nnx.Rngs(42),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8d4fa4a-3383-4bb0-bfbd-1dc193bc6888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=(7, 16, 24, 1) grid.mask.shape=(7, 16, 24)\n",
      "presence.shape=(7, 16, 24, 10) count.shape=(7, 10) grid.mask.shape=(7, 16, 24) prevalence.shape=(7, 10) intensity.shape=(7, 10)\n",
      "context.shapes=namespace(invariant=(7, 0, 0), space=(7, 0, 2, 1), flavour=(7, 0, 10, 2)) cells.shapes=namespace(invariant=(7, 16, 24, 2), space=(7, 16, 24, 4, 2), flavour=(7, 16, 24, 10, 1))\n",
      "pre_embedding.shapes=namespace(context=namespace(invariant=(7, 0, 0), space=(7, 0, 2, 1), flavour=(7, 0, 10, 2)), cells=namespace(invariant=(7, 16, 24, 2), space=(7, 16, 24, 4, 2), flavour=(7, 16, 24, 10, 1)), grid=namespace(xpos=(7, 24, 2), ypos=(7, 16, 2), mask=(7, 16, 24)))\n",
      "embedding.shapes=namespace(context=namespace(invariant=(7, 0, 224), space=(7, 0, 8, 32), flavour=(7, 0, 10, 16)), cells=namespace(invariant=(7, 16, 24, 176), space=(7, 16, 24, 8, 16), flavour=(7, 16, 24, 10, 8)), grid=namespace(xpos=(7, 24, 2), ypos=(7, 16, 2), mask=(7, 16, 24)))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'SplitSymDecomp' and 'Param'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m inp = jax.random.randint(enc.rngs(), Bs+(Y,X), \u001b[32m0\u001b[39m, \u001b[32m10\u001b[39m)\n\u001b[32m      5\u001b[39m size = jnp.tile(jnp.array((Y,X)),Bs+(\u001b[32m1\u001b[39m,))\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m out = \u001b[43menc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git-private/arc-2025/src/arc25/vision2/encoder.py:148\u001b[39m, in \u001b[36mARCEncoder.__call__\u001b[39m\u001b[34m(self, x, size, rngs, deterministic, unroll, remat, mode)\u001b[39m\n\u001b[32m    146\u001b[39m embedding = pre_embedding.map_projections(\u001b[38;5;28;01mlambda\u001b[39;00m v, f: f(v), \u001b[38;5;28mself\u001b[39m.embedding)\n\u001b[32m    147\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding.shapes\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m embedding = attrs.evolve(embedding, context=\u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontext_tokens\u001b[49m)\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# Apply the dropout layer to embedded patches.\u001b[39;00m\n\u001b[32m    151\u001b[39m embedding = embedding.map_representations(\n\u001b[32m    152\u001b[39m     \u001b[38;5;28mself\u001b[39m.dropout, rngs=rngs, deterministic=deterministic\n\u001b[32m    153\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for +: 'SplitSymDecomp' and 'Param'"
     ]
    }
   ],
   "source": [
    "Bs = (7,)\n",
    "Y,X = 16,24\n",
    "\n",
    "inp = jax.random.randint(enc.rngs(), Bs+(Y,X), 0, 10)\n",
    "size = jnp.tile(jnp.array((Y,X)),Bs+(1,))\n",
    "\n",
    "out = enc(inp,size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cc87dae-dc9a-4644-83fa-362591a0f6b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asdfas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43masdfas\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'asdfas' is not defined"
     ]
    }
   ],
   "source": [
    "asdfas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88c231a9-b644-4e2c-8b58-0390f28bb7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with lzma.LZMAFile(data_root/\"repack/re-arc.cbor.xz\",\"rb\") as fh:\n",
    "    src_data = serialisation.deserialise(cbor2.load(fh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfeadae7-4ff1-442f-94c9-993d49290d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x372a30c20>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "size_counts = np.zeros((31,31),int)\n",
    "for k,v in src_data.items():\n",
    "    for i,iop in enumerate(v):\n",
    "        for kk in [\"input\",\"output\"]:\n",
    "            vv = getattr(iop,kk)\n",
    "            if any(s>30 for s in vv.shape):\n",
    "                continue \n",
    "            h,w = vv.shape\n",
    "            size_counts[h,w] += 1\n",
    "\n",
    "plt.pcolormesh(size_counts,norm=\"log\")\n",
    "plt.axis(\"square\")\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b055ff8c-79ed-4e26-84a5-a2db1dd88d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With N=2, waste 199.2/422.9 M (efficiency 52.9 %) with [16 30]\n",
      "With N=3, waste 121.2/344.9 M (efficiency 64.9 %) with [12 21 30]\n",
      "With N=4, waste 86.3/310.0 M (efficiency 72.2 %) with [11 17 24 30]\n",
      "With N=5, waste 64.7/288.4 M (efficiency 77.6 %) with [ 8 14 20 25 30]\n",
      "With N=6, waste 50.7/274.4 M (efficiency 81.5 %) with [ 7 12 16 20 25 30]\n",
      "With N=7, waste 41.5/265.3 M (efficiency 84.3 %) with [ 6 11 15 19 23 26 30]\n"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "\n",
    "def brute(size_counts,N=4):\n",
    "    i = np.arange(0,31)\n",
    "    ccounts = np.cumsum(np.cumsum(size_counts,axis=0),axis=1)\n",
    "    ncells = i[:,None]*i\n",
    "    total_cells = size_counts*ncells\n",
    "    ccells = np.cumsum(np.cumsum(total_cells,axis=0),axis=1)\n",
    "    mx = 30\n",
    "    useful = ccells[mx,mx]\n",
    "    best, bestb = brute_impl(ccounts,mx,N)\n",
    "    return useful, best, bestb\n",
    "    \n",
    "@numba.njit\n",
    "def brute_impl(ccounts,mx,N):\n",
    "    best = 30**2*ccounts[-1,-1]\n",
    "    bestb = np.zeros(N,dtype=np.int_)\n",
    "    j = np.arange(N+1, dtype=np.int64)\n",
    "    j[-1] = mx\n",
    "    while True:\n",
    "        cc = ccounts[j,:][:,j]\n",
    "        bc = cc[1:]-cc[:-1]\n",
    "        bcnt = bc[:,1:]-bc[:,:-1]\n",
    "        bsz = j[1:,None]*j[1:]\n",
    "        spent = np.sum(bcnt*bsz)\n",
    "        if spent < best:\n",
    "            best = spent\n",
    "            bestb = j[1:].copy()\n",
    "\n",
    "        i = N-1\n",
    "        # find rightmost position that can be incremented\n",
    "        while i > 0 and j[i]+1 == j[i+1]:\n",
    "            i -= 1\n",
    "        if i <= 0:\n",
    "            # already at the last combination\n",
    "            break\n",
    "        j[i] += 1\n",
    "        # reset the tail to the minimal ascending values\n",
    "        while i+1<N:\n",
    "            j[i+1] = j[i] + 1\n",
    "            i += 1\n",
    "        \n",
    "    return best, bestb\n",
    "\n",
    "for N in range(2,8):\n",
    "    n_useful, n_spent, best = brute(size_counts, N=N)\n",
    "    print(f\"With {N=}, waste {(n_spent-n_useful)/1e6:.1f}/{n_spent/1e6:.1f} M (efficiency {100*n_useful/n_spent:.1f} %) with {best}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d849b25-dad7-461b-a0e4-97af2114f58e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m size_cuts = np.r_[\u001b[32m16\u001b[39m,\u001b[32m30\u001b[39m]\n\u001b[32m      5\u001b[39m skipped = []\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m tqdm.auto.tqdm(\u001b[43msrc_data\u001b[49m.items()):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i,iop \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(v):\n\u001b[32m      8\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m kk \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[31mNameError\u001b[39m: name 'src_data' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "#size_cuts = np.r_[11,17,24,30]\n",
    "#size_cuts = np.r_[8,12,16,24,30]\n",
    "size_cuts = np.r_[16,30]\n",
    "skipped = []\n",
    "for k,v in tqdm.auto.tqdm(src_data.items()):\n",
    "    for i,iop in enumerate(v):\n",
    "        for kk in [\"input\",\"output\"]:\n",
    "            vv = getattr(iop,kk)\n",
    "            if any(s>30 for s in vv.shape):\n",
    "                skipped.append(vv)\n",
    "                continue\n",
    "            gs = tuple(int(size_cuts[np.searchsorted(size_cuts,s)]) for s in vv.shape)\n",
    "            dataset.append(dict(\n",
    "                image = vv._data,\n",
    "                shape = vv.shape,\n",
    "                size = int(np.prod(vv.shape)),\n",
    "                grouping_shape = gs,\n",
    "                challenge = k,\n",
    "                type = kk,\n",
    "            ))\n",
    "print(f\"Skipped {len(skipped)} out of {len(skipped)+len(dataset)} due to out-of-range shape\")\n",
    "datasrc = pd.DataFrame(dataset)\n",
    "datasrc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f8caa81-c37b-4578-a2ad-20eccd1d2b0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasrc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m total_cells = \u001b[32m0\u001b[39m\n\u001b[32m      3\u001b[39m min_grp = \u001b[32m1000000\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m gs,grp \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdatasrc\u001b[49m.groupby(\u001b[33m\"\u001b[39m\u001b[33mgrouping_shape\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      5\u001b[39m     s = gs[\u001b[32m0\u001b[39m]*gs[\u001b[32m1\u001b[39m]*grp.shape[\u001b[32m0\u001b[39m]\n\u001b[32m      6\u001b[39m     n = grp[\u001b[33m\"\u001b[39m\u001b[33msize\u001b[39m\u001b[33m\"\u001b[39m].sum()\n",
      "\u001b[31mNameError\u001b[39m: name 'datasrc' is not defined"
     ]
    }
   ],
   "source": [
    "total_waste = 0\n",
    "total_cells = 0\n",
    "min_grp = 1000000\n",
    "for gs,grp in datasrc.groupby(\"grouping_shape\"):\n",
    "    s = gs[0]*gs[1]*grp.shape[0]\n",
    "    n = grp[\"size\"].sum()\n",
    "    util = n/s\n",
    "    waste = s-n\n",
    "    total_waste += waste\n",
    "    total_cells += n\n",
    "    min_grp = min(min_grp, grp.shape[0])\n",
    "    print(f\"Group {str(gs):8s} has {grp.shape[0]:6} with an average utilisation of {util*100:.0f} %, wasting {waste*1e-3:.1f}k cells\")\n",
    "print(f\"Total waste: {total_waste*1e-6:.1f}M cells vs {total_cells*1e-6:.1f}M useful cells\")\n",
    "print(f\"Maximum batch size / minimum group size: {min_grp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5953f000-c97a-4105-a73f-45d82ae9bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xa\n",
    "\n",
    "challenge_index = pd.CategoricalIndex(sorted(datasrc.challenge.unique()))\n",
    "itype_index = pd.CategoricalIndex(sorted(datasrc.type.unique()))\n",
    "challenge_index, itype_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d45510-d366-4989-843e-73bc629dcbe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "padded_data = {}\n",
    "for gs,grp in datasrc.groupby(\"grouping_shape\"):\n",
    "    n = grp.shape[0]\n",
    "    images = np.zeros((n,)+gs,\"i1\")\n",
    "    sizes = np.zeros((n,2),int)\n",
    "    challenges = np.zeros((n,),int)\n",
    "    itype = np.zeros((n,),int)\n",
    "    for i,(_,row) in enumerate(grp.iterrows()):\n",
    "        h,w = row[\"shape\"]\n",
    "        images[i,:h,:w] = row.image\n",
    "        sizes[i,:] = h,w\n",
    "        challenges[i] = challenge_index.get_loc(row.challenge)\n",
    "        itype[i] = itype_index.get_loc(row.type)\n",
    "    data = xa.Dataset(\n",
    "        dict(\n",
    "            images = ((\"idx\",\"row\",\"col\"), images),\n",
    "            sizes = ((\"idx\",\"dim\"), sizes),\n",
    "            challenges = ((\"idx\",),challenges),\n",
    "            itype = ((\"idx\",),itype),\n",
    "        ),\n",
    "        coords = dict(\n",
    "            dim = pd.Index([\"row\",\"col\"]),\n",
    "        ),\n",
    "    )\n",
    "    padded_data[gs] = data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c05cc-d8ec-4bdb-a8de-3d2333b0eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(batch_size: int, rngs:nnx.Rngs):\n",
    "    weights = {}\n",
    "    for gs,data in padded_data.items():\n",
    "        weights[gs] = data.idx.size\n",
    "    logits = np.log(np.array(list(weights.values())))\n",
    "    keys = tuple(weights.keys())\n",
    "    rng = rngs\n",
    "    seen = set()\n",
    "    while True:\n",
    "        gs = keys[rng.categorical(logits)]\n",
    "        is_first_of_kind = gs not in seen\n",
    "        seen.add(gs)\n",
    "        data = padded_data[gs]\n",
    "        assert data.idx.size >= batch_size\n",
    "        i = set()\n",
    "        while len(i) < batch_size:\n",
    "            j = rng.randint((batch_size-len(i),),0,data.idx.size)\n",
    "            i.update(int(v) for v in j)\n",
    "        i = np.array(sorted(i))\n",
    "        images = data.images.to_numpy()[i]\n",
    "        sizes = data[\"sizes\"].to_numpy()[i]\n",
    "        labels = data.challenges.to_numpy()[i]\n",
    "        yield dict(\n",
    "            inputs = dict(image=images, size=sizes),\n",
    "            label = labels,\n",
    "        ), is_first_of_kind, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beebdf2-e4ab-4908-943a-21cc6826bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2820c741-633e-4a27-b768-8aca53432dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ModelConfig:\n",
    "    num_classes: int = 1000\n",
    "    embed_dim: int = 256\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainConfig:\n",
    "    \"\"\"Configuration for the training script.\"\"\"\n",
    "    seed: int = 42\n",
    "    global_batch_size: int = 128  # across all devices\n",
    "    ref_batch: int = 1024 # all learning rates refer to this batch size\n",
    "    ref_epoch: int = 800000 # number of images that we call one epoch in reporting\n",
    "    # Optimiser\n",
    "    learning_rate: float = 3e-4\n",
    "    betas: tuple[float, float] = (0.9, 0.999)\n",
    "    eps: float = 1e-8\n",
    "    weight_decay: float = 0.05\n",
    "    grad_clip_norm: float = 1.0\n",
    "    # Schedule    \n",
    "    # in units of ref_batch images!\n",
    "    num_train_steps: int = 1000\n",
    "    warmup_steps: float = 10\n",
    "    log_every_steps: int = 50\n",
    "\n",
    "    # TODO: EMA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9fef38-c371-43f1-a1c5-0b8850239c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrainState(nnx.Module):\n",
    "    \"\"\"A frozen dataclass to hold the training state.\"\"\"\n",
    "    config: TrainConfig = dataclasses.field(metadata=dict(static=True))\n",
    "    model: nnx.Module\n",
    "    optimizer: nnx.Optimizer\n",
    "    # ema_params: nnx.Params\n",
    "\n",
    "    @classmethod\n",
    "    def make(\n",
    "        cls,\n",
    "        model: nnx.Module,\n",
    "        config: TrainConfig,\n",
    "        *,\n",
    "        rngs: nnx.Rngs,\n",
    "    ) -> typing.Self:\n",
    "        \"\"\"Initializes the model, optimizer, and the combined training state.\"\"\"\n",
    "\n",
    "        step_scale = config.ref_batch / config.global_batch_size\n",
    "        epoch_scale = config.ref_epoch / config.global_batch_size\n",
    "        # Create the learning rate schedule (warmup + cosine decay is standard for ViTs)\n",
    "        # with Linear LR scaling\n",
    "        lr = config.learning_rate / step_scale\n",
    "        zero_lr = lr * 0.001\n",
    "        lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "            init_value=zero_lr,\n",
    "            peak_value=config.learning_rate,\n",
    "            warmup_steps=int(round(config.warmup_steps*step_scale)),\n",
    "            decay_steps=int(round((config.num_train_steps - config.warmup_steps)*step_scale)),\n",
    "            end_value=zero_lr,\n",
    "        )\n",
    "    \n",
    "        # Create the AdamW optimizer with gradient clipping\n",
    "        tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config.grad_clip_norm),\n",
    "            optax.adamw(\n",
    "                learning_rate=lr_schedule,\n",
    "                weight_decay=config.weight_decay,\n",
    "                b1=config.betas[0],\n",
    "                b2=config.betas[1], \n",
    "                eps=config.eps,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self = cls()\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.optimizer = nnx.Optimizer(model, tx, wrt=nnx.Param)\n",
    "        self.stats = {k:nnx.Variable(0) for k in [\"steps\",\"examples\"]}\n",
    "\n",
    "        return self\n",
    "\n",
    "    def model_stats(self):\n",
    "        stats = {}\n",
    "        for k in [\"model\",\"optimizer\"]:\n",
    "            _, v = nnx.split(getattr(self, k))\n",
    "            leaves = jax.tree_util.tree_leaves(v)\n",
    "            total_params = sum((leaf.size for leaf in leaves), start=0)\n",
    "            total_bytes = sum((leaf.nbytes for leaf in leaves), start=0)\n",
    "            stats[k] = SimpleNamespace(params=total_params,bytes=total_bytes)\n",
    "        \n",
    "        return SimpleNamespace(**stats)\n",
    "\n",
    "    @classmethod\n",
    "    def loss_fn(cls, model, batch, **kw):\n",
    "        inputs = batch[\"inputs\"]\n",
    "        logits = model(inputs[\"image\"], inputs[\"size\"], **kw)\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "            logits=logits, labels=batch[\"label\"],\n",
    "            #label_smoothing=cfg.label_smoothing,\n",
    "        ).mean()\n",
    "        accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == batch[\"label\"])\n",
    "        return loss, dict(loss=loss, accuracy=accuracy)\n",
    "\n",
    "    def batch_stats(self, batch_size:int|None=None,shape:tuple[int,int] = (30,30)):\n",
    "        graph, state = nnx.split(self)\n",
    "        def inference(state, batch):\n",
    "            state = nnx.merge(graph, state)\n",
    "            model = state.model\n",
    "            model.eval()\n",
    "            inputs = batch[\"inputs\"]\n",
    "            return model(inputs[\"image\"], inputs[\"size\"])\n",
    "        def train(state, batch):\n",
    "            state = nnx.merge(graph, state)\n",
    "            model = state.model\n",
    "            model.train()\n",
    "            def loss_fn(model):\n",
    "                return self.loss_fn(model, batch)\n",
    "            grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "            (_, stats), grads = grad_fn(model)\n",
    "\n",
    "            # Update optimizer state and compute new parameters\n",
    "            state.optimizer.update(state.model, grads)\n",
    "            _, state = nnx.split(state)\n",
    "            return state, stats\n",
    "            \n",
    "        if batch_size is None:\n",
    "            batch_size = self.config.global_batch_size\n",
    "        batch = dict(\n",
    "            inputs = dict(\n",
    "                image = np.zeros((batch_size,)+shape,\"i1\"),\n",
    "                size = np.tile(shape,(batch_size,1)),\n",
    "            ),\n",
    "            # TODO: num classes!\n",
    "            label = np.arange(batch_size,dtype=int)%400,\n",
    "        )\n",
    "        stats = {}\n",
    "        for k,fun in dict(\n",
    "            inference=inference,\n",
    "            train=train,\n",
    "        ).items():\n",
    "            # Analyze the forward pass function\n",
    "            jfun = jax.jit(fun)\n",
    "            tfun = jfun.trace(state, batch)\n",
    "            cfun = tfun.lower()#.compile()\n",
    "            cost = cfun.cost_analysis()\n",
    "            stats[k] = SimpleNamespace(\n",
    "                flops = cost.get(\"flops\"),\n",
    "                bytes_accessed = cost.get(\"bytes accessed\"),\n",
    "                bytes_out = cost.get(\"bytes accessedout\"),\n",
    "            )\n",
    "            \n",
    "        return SimpleNamespace(**stats)\n",
    "\n",
    "    def train_step(self, batch, num_devices, **kw):\n",
    "        @nnx.split_rngs(splits=num_devices)\n",
    "        def wrapper(state, batch):\n",
    "            return state._parallel_train_step(batch, tuple(sorted(kw.items())))\n",
    "\n",
    "        batch = jax.tree.map(lambda x:x.reshape(num_devices,-1,*x.shape[1:]), batch)\n",
    "        stats = wrapper(self, batch)\n",
    "\n",
    "        # TODO: this one probably defeats latency hiding\n",
    "        stats = jax.device_get(jax.tree.map(lambda x: x[0], stats))\n",
    "        self.stats[\"steps\"] += 1\n",
    "        self.stats[\"examples\"] += batch[\"label\"].shape[0]\n",
    "        stats.update({k:v.value for k,v in self.stats.items()})\n",
    "        return stats\n",
    "\n",
    "        \n",
    "    @nnx.pmap(\n",
    "        axis_name=\"data\",\n",
    "        in_axes=(nnx.StateAxes({('data',nnx.RngKey,nnx.RngCount): 0, ...: None}),0,None),\n",
    "        static_broadcasted_argnums=2,\n",
    "    )\n",
    "    def _parallel_train_step(self, batch, kw):\n",
    "        def loss_fn(model):\n",
    "            return self.loss_fn(model, batch, **dict(kw))\n",
    "\n",
    "        grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "        (_, stats), grads = grad_fn(self.model)\n",
    "\n",
    "        grads = jax.lax.pmean(grads, axis_name='data')\n",
    "        stats = jax.lax.pmean(stats, axis_name='data')\n",
    "        \n",
    "        self.optimizer.update(self.model, grads)\n",
    "        return stats        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7de7ffd-0929-4f3e-b4c7-909087fa4e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "def main(model, config: TrainConfig, **kw):\n",
    "    \"\"\"The main entry point for the training script.\"\"\"\n",
    "    # Detect available devices (GPUs/TPUs)\n",
    "    num_devices = jax.local_device_count()\n",
    "    is_multi_device = num_devices > 1\n",
    "\n",
    "    per_device_batch_size = config.global_batch_size // num_devices\n",
    "    config = dataclasses.replace(config, global_batch_size=per_device_batch_size*num_devices)\n",
    "\n",
    "    step_scale = config.ref_batch / config.global_batch_size\n",
    "    epoch_scale = config.ref_epoch / config.global_batch_size\n",
    "    \n",
    "    print(f\"--- ARC ViT Pre-training ---\")\n",
    "    print(f\"Detected {num_devices} devices: {jax.devices()}\")\n",
    "    print(f\"Mode: {'Single-device' if not is_multi_device else 'Multi-device (DDP)'}\")\n",
    "    print(f\"Per-device batch size: {per_device_batch_size}\")\n",
    "    print(f\"Global batch size: {config.global_batch_size}\")\n",
    "    print(\"----------------------------\\n\")\n",
    "\n",
    "    # Setup PRNG key\n",
    "    rngs = nnx.Rngs(config.seed)\n",
    "\n",
    "    # 1. Setup Data Pipeline\n",
    "    # ...\n",
    "\n",
    "    # 2. Initialize Training State\n",
    "    state = TrainState.make(\n",
    "        model,\n",
    "        config,\n",
    "        rngs=rngs,\n",
    "    )\n",
    "    \n",
    "    # 4. Start the training loop\n",
    "    print(\"Starting training...\")\n",
    "    start_time = time.monotonic()\n",
    "    prev_images = jit_imgs = 0\n",
    "    prev_elapsed = jit_time = 0\n",
    "    \n",
    "    # Get an iterator for the dataset\n",
    "    ds_iter = iter(make_dataset(config.global_batch_size, rngs=rngs))\n",
    "\n",
    "    try:\n",
    "        was_warmup = True\n",
    "        stats = {}\n",
    "        for step in (pbar := tqdm.auto.trange(int(round(config.num_train_steps*step_scale)))):\n",
    "            # Fetch the next batch of data\n",
    "            batch,is_first_of_kind,epoch = next(ds_iter)\n",
    "            \n",
    "            # Execute one parallel training step\n",
    "            s = state.train_step(batch, num_devices, **kw)\n",
    "    \n",
    "            elapsed_time = time.monotonic() - start_time\n",
    "            images = (step+1)*config.global_batch_size\n",
    "            if is_first_of_kind:\n",
    "                jit_time += elapsed_time - prev_elapsed\n",
    "                jit_imgs += images - prev_images\n",
    "            prev_elapsed = elapsed_time\n",
    "            prev_images = images\n",
    "            images_per_sec = (images-jit_imgs) / (elapsed_time-jit_time) if elapsed_time>jit_time+1 else images/elapsed_time\n",
    "            info = dict(\n",
    "                #step=step,\n",
    "                epoch = epoch,\n",
    "                refstep = images/config.ref_batch,\n",
    "                refepoch= images/config.ref_epoch,\n",
    "                images = images,\n",
    "                images_per_sec=images_per_sec,\n",
    "                seconds_per_epoch=config.ref_epoch/images_per_sec,\n",
    "                **s\n",
    "            )\n",
    "            for k,v in info.items():\n",
    "                stats.setdefault(k,[]).append(v)\n",
    "            pbar.set_postfix(**{\n",
    "                dict(images_per_sec=\"ips\").get(k,k):f\"{info[k]:.{v}f}\"\n",
    "                for k,v in dict(\n",
    "                    loss=2,\n",
    "                    accuracy=3,\n",
    "                    images_per_sec=2,\n",
    "                    refepoch=2,\n",
    "                    epoch=0,\n",
    "                    refstep=2,\n",
    "                ).items()\n",
    "            })\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    print(\"\\n--- Training Finished ---\")\n",
    "    stats = {k:np.array(v) for k,v in stats.items()}\n",
    "    stats = pd.DataFrame(stats)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c36d2-3558-4cec-91b0-2beeec1703a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = FieldDims.make(\n",
    "    inv_fac = 2,\n",
    "    context = 32,\n",
    "    hdrs = 16,\n",
    "    cells = 16,\n",
    ")\n",
    "arc_cls = ARCClassifier(\n",
    "    hidden_size = dims,\n",
    "    mha_features = 24*2,\n",
    "    mlp_width_factor = 2,\n",
    "    num_heads = 2,\n",
    "    num_groups = 1,\n",
    "    num_classes = 400,\n",
    "    num_layers = 8,\n",
    "    rngs = nnx.Rngs(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf7fff-a6e3-4c39-ba76-d99006f0bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainConfig(\n",
    "    global_batch_size = 16,\n",
    "    num_train_steps = 1000,\n",
    "    warmup_steps = 5,\n",
    "    learning_rate = 3e-4,\n",
    "    weight_decay = 0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8d1c7bd-c02c-47cd-a987-f9ea1c19a8bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dims' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m state = jax.tree.map(\u001b[38;5;28;01mlambda\u001b[39;00m v:v[\u001b[32m0\u001b[39m],state)\n\u001b[32m      8\u001b[39m layer = nnx.merge(gs,state)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m x = \u001b[43mdims\u001b[49m.make_empty(batch=(\u001b[32m1\u001b[39m,),shape=(\u001b[32m30\u001b[39m,\u001b[32m30\u001b[39m),flavours=\u001b[32m11\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_fun\u001b[39m(model,x,**kw):\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model(x,**kw)\n",
      "\u001b[31mNameError\u001b[39m: name 'dims' is not defined"
     ]
    }
   ],
   "source": [
    "import humanize\n",
    "\n",
    "if True:\n",
    "#    layer = arc_cls.encoder.blocks\n",
    "    layer = enc.blocks\n",
    "    gs,state = nnx.split(layer)\n",
    "    state = jax.tree.map(lambda v:v[0],state)\n",
    "    layer = nnx.merge(gs,state)\n",
    "    \n",
    "    x = dims.make_empty(batch=(1,),shape=(30,30),flavours=11)\n",
    "    def test_fun(model,x,**kw):\n",
    "        return model(x,**kw)\n",
    "    def unpack(obj,path=()):\n",
    "        match obj:\n",
    "            case SimpleNamespace():\n",
    "                yield from unpack(vars(obj),path)\n",
    "            case dict():\n",
    "                for k,v in obj.items():\n",
    "                    yield from unpack(v,path+(k,))\n",
    "            case SymmetricLinear():\n",
    "                yield path,obj.approximate_flops\n",
    "            case nnx.Linear():\n",
    "                yield path,obj.kernel.size\n",
    "            case _:\n",
    "                print(f\"Ignoring {type(obj).__name__}\")\n",
    "    attn = layer.attn\n",
    "    tot = 0\n",
    "    for k,v in unpack(dict(mix=attn.cell_global_mix,qkv=attn.qkv,out=attn.out)):\n",
    "        F = 11\n",
    "        N = 30\n",
    "        fac = dict(\n",
    "            mix=F,\n",
    "            context=F,\n",
    "            rows=F*N,\n",
    "            cols=F*N,\n",
    "            cells=F*N*N,\n",
    "        ).get(k[-1])\n",
    "        tot += v*fac\n",
    "        print(f\"{'.'.join(k)}: {humanize.naturalsize(v*fac)}\")\n",
    "    print(f\"Total attention projections: {humanize.naturalsize(tot)}\")\n",
    "    compiled = nnx.jit(test_fun).trace(layer.attn,x,deterministic=True).lower().compile()\n",
    "    print(f\"Total attn: {humanize.naturalsize(compiled.cost_analysis()[\"flops\"])}\")\n",
    "    compiled = nnx.jit(test_fun).trace(layer.mlp,x,rngs=nnx.Rngs(0)).lower().compile()\n",
    "    print(f\"Total MLP: {humanize.naturalsize(compiled.cost_analysis()[\"flops\"])}\")\n",
    "    compiled = nnx.jit(test_fun).trace(layer,x,rngs=nnx.Rngs(0)).lower().compile()\n",
    "    print(f\"Total Layer: {humanize.naturalsize(compiled.cost_analysis()[\"flops\"])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254284da-1dac-4958-8642-041bea326a38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import humanize\n",
    "\n",
    "def print_stats(stats,path=()):\n",
    "    for k,v in vars(stats).items():\n",
    "        p = path+(k,)\n",
    "        if isinstance(v, SimpleNamespace):\n",
    "            print_stats(v, p)\n",
    "            continue\n",
    "        if v is None:\n",
    "            continue\n",
    "        if p[0] == \"model\":\n",
    "            va = {\"\":v}\n",
    "        else:\n",
    "            va = dict(batch=v,example=v/config.global_batch_size)\n",
    "        msg = []\n",
    "        for kk,vv in va.items():\n",
    "            if \"bytes\" in k:\n",
    "                n = humanize.naturalsize(vv,binary=True)\n",
    "            else:\n",
    "                n = humanize.naturalsize(vv,gnu=True,format=\"%.1f \")\n",
    "                if n.endswith(\"B\"):\n",
    "                    n = n[:-1]+\" \"\n",
    "            if \"flops\" in k:\n",
    "                n += \"FLOPs\"\n",
    "            if kk:\n",
    "                n += f\" ({kk})\"\n",
    "            msg.append(n)\n",
    "        n = \"\".join(f\"{n:25s}\" for n in msg)\n",
    "        print(f\"{'.'.join(p):32s}: {n}\")\n",
    "\n",
    "\n",
    "ts = TrainState.make(model=arc_cls, config=config, rngs=nnx.Rngs(0))\n",
    "stats = SimpleNamespace(model=ts.model_stats(),batch=ts.batch_stats())\n",
    "leaves = [\n",
    "    a for p in dims.make_empty(\n",
    "        batch=(config.global_batch_size,),\n",
    "        shape=(30,30),\n",
    "        flavours=11,\n",
    "    ).projections.values() for a in p.representations.values()\n",
    "]\n",
    "stats.fields = SimpleNamespace(params=sum(a.size for a in leaves),bytes=sum(a.nbytes for a in leaves))\n",
    "print_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb2cae-8adb-437a-ae5a-6eed6451f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = main(arc_cls, config, remat=True, unroll=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4fd383-a3ff-46ac-8965-c906a4d0f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "\n",
    "def filt(signal, filter_width = 50):\n",
    "    signal = ndimage.uniform_filter1d(signal, filter_width, mode=\"constant\")\n",
    "    signal /= ndimage.uniform_filter1d(np.ones_like(signal), filter_width, mode=\"constant\")\n",
    "    return signal\n",
    "\n",
    "fig,axes = plt.subplots(2,1,figsize=(8,8))\n",
    "ax = axes[0]\n",
    "ax.semilogy(stats.refstep, filt(stats.loss))\n",
    "ax = axes[1]\n",
    "ax.plot(stats.refstep, filt(stats.accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ac2226-c964-49c9-b435-7bde74ee485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import msgpack\n",
    "import lzma\n",
    "import anyio\n",
    "import pickle\n",
    "\n",
    "asdfasd\n",
    "\n",
    "async with await anyio.open_file(data_root/\"arc-vision-model.msgpack.xz\",\"wb\") as fh:\n",
    "    compressor = lzma.LZMACompressor()\n",
    "    \n",
    "    async def write(*data):\n",
    "        serialised = msgpack.dumps(tuple(data))\n",
    "        compressed = await anyio.to_thread.run_sync(compressor.compress,serialised)\n",
    "        await fh.write(compressed)\n",
    "    \n",
    "    graphdef,flatstate = nnx.graph.flatten(nnx.pure(nnx.state(arc_cls, nnx.Param)))\n",
    "    await write(\"G\",pickle.dumps(graphdef))\n",
    "    pointer = ()\n",
    "    for path,s in tqdm.auto.tqdm(flatstate):\n",
    "        pstr = \".\".join(str(v) for v in path)\n",
    "        assert isinstance(s, jaxlib._jax.ArrayImpl)\n",
    "        d = np.asarray(jax.device_get(s))\n",
    "        neq = 0\n",
    "        for i,(a,b) in enumerate(zip(pointer, path)):\n",
    "            neq = i\n",
    "            if a != b:\n",
    "                break\n",
    "        key = (neq,path[neq:])\n",
    "        await write(\"A\",neq,path[neq:],d.shape,str(d.dtype),bytes(d.data))\n",
    "        pointer = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a405c6-492d-44cd-8b07-68638528ad0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
