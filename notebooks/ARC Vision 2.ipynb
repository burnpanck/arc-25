{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e92cc294-fc57-4ca6-a250-317eeeba70b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import lzma\n",
    "import os\n",
    "from pathlib import Path\n",
    "from types import MappingProxyType, SimpleNamespace\n",
    "\n",
    "import cbor2\n",
    "import attrs\n",
    "import tqdm.auto\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "import optax\n",
    "import jaxtyping as jt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from arc25 import symmetry\n",
    "from arc25.symmetry import D4, transform_vector\n",
    "from arc25 import serialisation\n",
    "from arc25.dsl.types import Vector, Dir4\n",
    "from arc25.vision2.symrep import SymDecompBase, SplitSymDecomp, SymDecompDims, standard_rep, RepSpec\n",
    "from arc25.vision2.linear import SpaceSymmetricLinear, SpaceSymmetricTensor, SymmetryMappingSpec, SymDecompLinear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62411ecc-c33b-4af9-b2b6-71b6674fd2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"XLA_FLAGS\"]=\"--xla_force_host_platform_device_count=2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b090806-d291-4fa3-adfd-da0e9594a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# helper functions\n",
    "#----------------------\n",
    "def pytree_structure(pytree, title='pytree structure'):\n",
    "  print(f\"{title}:\")\n",
    "  for path, value in jax.tree.leaves_with_path(pytree):\n",
    "    print(f\"- pytree{jax.tree_util.keystr(path)} = {value.shape!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ef38c9d-a501-4a8f-8b04-540553241b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytree structure:\n",
      "- pytree.bias.params.value = (8, 9)\n",
      "- pytree.kernel.params.value = (64, 11, 9)\n"
     ]
    }
   ],
   "source": [
    "lin = SpaceSymmetricLinear(\n",
    "    (symmetry.FullRep,),\n",
    "    11,\n",
    "    (symmetry.FullRep,symmetry.WindingRep,symmetry.AxialDirRep),\n",
    "    9,\n",
    "    rngs=nnx.Rngs(0),\n",
    ")\n",
    "pytree_structure(lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "57fcf93b-7d36-482a-9358-983cdaa0811a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnx.VariableState(np.random.randint((12,12))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22760d7f-6769-4e41-bebf-366e116e1c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytree structure:\n",
      "- pytree.bias.flavour.params.value = (1, 7)\n",
      "- pytree.bias.invariant.params.value = (1, 19)\n",
      "- pytree.bias.space.params.value = (1, 13)\n",
      "- pytree.flavour_invariant.flavour.flavour.kernel.params.value = (1, 5, 7)\n",
      "- pytree.flavour_invariant.flavour.invariant.kernel.params.value = (1, 5, 19)\n",
      "- pytree.flavour_invariant.invariant.flavour.kernel.params.value = (1, 17, 7)\n",
      "- pytree.flavour_invariant.invariant.invariant.kernel.params.value = (1, 17, 19)\n",
      "- pytree.flavour_invariant.invariant.space.kernel.params.value = (4, 17, 13)\n",
      "- pytree.flavour_invariant.space.invariant.kernel.params.value = (4, 11, 19)\n",
      "- pytree.flavour_invariant.space.space.kernel.params.value = (32, 11, 13)\n",
      "- pytree.flavour_pointwise.kernel.params.value = (1, 5, 7)\n",
      "- pytree.rngs.default.count.value = ()\n",
      "- pytree.rngs.default.key.value = ()\n"
     ]
    }
   ],
   "source": [
    "inpf = SymDecompDims(17,11,5)\n",
    "outf = SymDecompDims(19,13,7)\n",
    "lin = SymDecompLinear(\n",
    "    inpf,\n",
    "    outf,\n",
    "    extra_in_reps=(symmetry.AxialDirRep,),\n",
    "    rngs=nnx.Rngs(0),\n",
    ")\n",
    "pytree_structure(lin)\n",
    "for k,v in nnx.flatten(nnx.state(lin, nnx.Param))[1]:\n",
    "    v[...] = jax.random.randint(lin.rngs.params(),v.shape,-2,3)#/2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "936b6bdd-ba20-4113-ba9a-3358aaaf645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = SplitSymDecomp.empty(inpf,batch=(3,4))\n",
    "for k,v in inp.representations.items():\n",
    "    s = dict(invariant=1,space=8,flavour=10)[k]\n",
    "    v[...] = s*jax.random.randint(lin.rngs.params(),v.shape,-3,4)/2\n",
    "out = lin(inp)\n",
    "for op in D4:\n",
    "    tfo = lambda rep:symmetry.transform_rep_idx(op,rep)\n",
    "    atfo = tfo(symmetry.AxialDirRep)\n",
    "    ftfo = tfo(symmetry.FullRep)\n",
    "    tinp = attrs.evolve(\n",
    "        inp,\n",
    "        invariant = inp.invariant[:,atfo,:],\n",
    "        flavour = inp.flavour[:,atfo,:,:],\n",
    "        space = inp.space[:,atfo[:,None],ftfo,:],\n",
    "    )\n",
    "    assert inpf.validate(tinp), inpf.validation_problems(tinp)\n",
    "    expected = attrs.evolve(\n",
    "        out,\n",
    "        space = out.space[:,ftfo,:],\n",
    "    )\n",
    "    assert outf.validate(expected), outf.validation_problems(expected)\n",
    "    actual = lin(tinp)\n",
    "    for k,v in expected.representations.items():\n",
    "        a = getattr(actual,k)\n",
    "        assert np.allclose(a, v), f\"{op}: {k}\"\n",
    "    for swapiter in range(10):\n",
    "        swp = np.r_[:10]\n",
    "        i,j = jax.random.randint(lin.rngs(),2,0,10)\n",
    "        swp[[i,j]] = swp[[j,i]]\n",
    "        tinp = attrs.evolve(\n",
    "            tinp,\n",
    "            flavour = tinp.flavour[:,:,swp,:],\n",
    "        )\n",
    "        expected = attrs.evolve(\n",
    "            expected,\n",
    "            flavour = expected.flavour[:,swp,:],\n",
    "        )\n",
    "        actual = lin(tinp)\n",
    "        for k,v in expected.representations.items():\n",
    "            a = getattr(actual,k)\n",
    "            assert np.allclose(a, v), f\"{op}/{swapiter} ({swp}): {k}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e61ec063-a0bf-427f-b094-6bbc3fdf08c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43masdf\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'asdf' is not defined"
     ]
    }
   ],
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5511f113-eba2-4ac1-8dd3-746f91e94dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "8*6+10*1-80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c81310f-dac0-4e78-8b1c-4a4071a66768",
   "metadata": {},
   "outputs": [],
   "source": [
    "qk = SymDecompLinear(\n",
    "    SymDecompDims(22*8,16,8),\n",
    "    SymDecompDims(22*4*3,24*3,4*3,rep=RepSpec(space=symmetry.AxialDirRep,n_flavours=10)),\n",
    "    use_bias=False,\n",
    "    rngs=nnx.Rngs(0),\n",
    ")\n",
    "pytree_structure(qk)\n",
    "\n",
    "v = SymDecompLinear(\n",
    "    SymDecompDims(22*8,16,8),\n",
    "    SymDecompDims(6*8,16,8),\n",
    "    extra_out_reps=(symmetry.AxialDirRep,),\n",
    "    rngs=nnx.Rngs(0),\n",
    ")\n",
    "pytree_structure(v)\n",
    "\n",
    "out = SymDecompLinear(\n",
    "    SymDecompDims(6*8,16,8),\n",
    "    SymDecompDims(22*8,16,8),\n",
    "    extra_in_reps=(symmetry.AxialDirRep,),\n",
    "    use_bias=False,\n",
    "    rngs=nnx.Rngs(0),\n",
    ")\n",
    "pytree_structure(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d05b34-c0d9-4ba4-8134-de9cb50818c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for spec in SymmetryMappingSpec._cache.values():\n",
    "    print(\n",
    "        f\"{\"[\"+\",\".join(f\"{ax.rep.__name__}:{\"co\" if ax.covariant else \"contra\"}\" for ax in spec.axes)+\"]\":64s}\"\n",
    "        f\" = {str(spec.permutation_index.shape):12s} {spec.n_free_subspaces:3d} ({spec.fully_coupled=:1} {spec.fully_independent=:1})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc87dae-dc9a-4644-83fa-362591a0f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_root = Path(\"..\").resolve()\n",
    "data_root = proj_root / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c231a9-b644-4e2c-8b58-0390f28bb7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with lzma.LZMAFile(data_root/\"repack/re-arc.cbor.xz\",\"rb\") as fh:\n",
    "    src_data = serialisation.deserialise(cbor2.load(fh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeadae7-4ff1-442f-94c9-993d49290d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "size_counts = np.zeros((31,31),int)\n",
    "for k,v in src_data.items():\n",
    "    for i,iop in enumerate(v):\n",
    "        for kk in [\"input\",\"output\"]:\n",
    "            vv = getattr(iop,kk)\n",
    "            if any(s>30 for s in vv.shape):\n",
    "                continue \n",
    "            h,w = vv.shape\n",
    "            size_counts[h,w] += 1\n",
    "\n",
    "plt.pcolormesh(size_counts,norm=\"log\")\n",
    "plt.axis(\"square\")\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055ff8c-79ed-4e26-84a5-a2db1dd88d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "\n",
    "def brute(size_counts,N=4):\n",
    "    i = np.arange(0,31)\n",
    "    ccounts = np.cumsum(np.cumsum(size_counts,axis=0),axis=1)\n",
    "    ncells = i[:,None]*i\n",
    "    total_cells = size_counts*ncells\n",
    "    ccells = np.cumsum(np.cumsum(total_cells,axis=0),axis=1)\n",
    "    mx = 30\n",
    "    useful = ccells[mx,mx]\n",
    "    best, bestb = brute_impl(ccounts,mx,N)\n",
    "    return useful, best, bestb\n",
    "    \n",
    "@numba.njit\n",
    "def brute_impl(ccounts,mx,N):\n",
    "    best = 30**2*ccounts[-1,-1]\n",
    "    bestb = np.zeros(N,dtype=np.int_)\n",
    "    j = np.arange(N+1, dtype=np.int64)\n",
    "    j[-1] = mx\n",
    "    while True:\n",
    "        cc = ccounts[j,:][:,j]\n",
    "        bc = cc[1:]-cc[:-1]\n",
    "        bcnt = bc[:,1:]-bc[:,:-1]\n",
    "        bsz = j[1:,None]*j[1:]\n",
    "        spent = np.sum(bcnt*bsz)\n",
    "        if spent < best:\n",
    "            best = spent\n",
    "            bestb = j[1:].copy()\n",
    "\n",
    "        i = N-1\n",
    "        # find rightmost position that can be incremented\n",
    "        while i > 0 and j[i]+1 == j[i+1]:\n",
    "            i -= 1\n",
    "        if i <= 0:\n",
    "            # already at the last combination\n",
    "            break\n",
    "        j[i] += 1\n",
    "        # reset the tail to the minimal ascending values\n",
    "        while i+1<N:\n",
    "            j[i+1] = j[i] + 1\n",
    "            i += 1\n",
    "        \n",
    "    return best, bestb\n",
    "\n",
    "for N in range(2,8):\n",
    "    n_useful, n_spent, best = brute(size_counts, N=N)\n",
    "    print(f\"With {N=}, waste {(n_spent-n_useful)/1e6:.1f}/{n_spent/1e6:.1f} M (efficiency {100*n_useful/n_spent:.1f} %) with {best}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d849b25-dad7-461b-a0e4-97af2114f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "#size_cuts = np.r_[11,17,24,30]\n",
    "size_cuts = np.r_[16,30]\n",
    "skipped = []\n",
    "for k,v in tqdm.auto.tqdm(src_data.items()):\n",
    "    for i,iop in enumerate(v):\n",
    "        for kk in [\"input\",\"output\"]:\n",
    "            vv = getattr(iop,kk)\n",
    "            if any(s>30 for s in vv.shape):\n",
    "                skipped.append(vv)\n",
    "                continue\n",
    "            gs = tuple(int(size_cuts[np.searchsorted(size_cuts,s)]) for s in vv.shape)\n",
    "            dataset.append(dict(\n",
    "                image = vv._data,\n",
    "                shape = vv.shape,\n",
    "                size = int(np.prod(vv.shape)),\n",
    "                grouping_shape = gs,\n",
    "                challenge = k,\n",
    "                type = kk,\n",
    "            ))\n",
    "print(f\"Skipped {len(skipped)} out of {len(skipped)+len(dataset)} due to out-of-range shape\")\n",
    "datasrc = pd.DataFrame(dataset)\n",
    "datasrc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8caa81-c37b-4578-a2ad-20eccd1d2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_waste = 0\n",
    "total_cells = 0\n",
    "min_grp = 1000000\n",
    "for gs,grp in datasrc.groupby(\"grouping_shape\"):\n",
    "    s = gs[0]*gs[1]*grp.shape[0]\n",
    "    n = grp[\"size\"].sum()\n",
    "    util = n/s\n",
    "    waste = s-n\n",
    "    total_waste += waste\n",
    "    total_cells += n\n",
    "    min_grp = min(min_grp, grp.shape[0])\n",
    "    print(f\"Group {str(gs):8s} has {grp.shape[0]:6} with an average utilisation of {util*100:.0f} %, wasting {waste*1e-3:.1f}k cells\")\n",
    "print(f\"Total waste: {total_waste*1e-6:.1f}M cells vs {total_cells*1e-6:.1f}M useful cells\")\n",
    "print(f\"Maximum batch size / minimum group size: {min_grp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5953f000-c97a-4105-a73f-45d82ae9bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xa\n",
    "\n",
    "challenge_index = pd.CategoricalIndex(sorted(datasrc.challenge.unique()))\n",
    "itype_index = pd.CategoricalIndex(sorted(datasrc.type.unique()))\n",
    "challenge_index, itype_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d45510-d366-4989-843e-73bc629dcbe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "padded_data = {}\n",
    "for gs,grp in datasrc.groupby(\"grouping_shape\"):\n",
    "    n = grp.shape[0]\n",
    "    images = np.zeros((n,)+gs,\"i1\")\n",
    "    sizes = np.zeros((n,2),int)\n",
    "    challenges = np.zeros((n,),int)\n",
    "    itype = np.zeros((n,),int)\n",
    "    for i,(_,row) in enumerate(grp.iterrows()):\n",
    "        h,w = row[\"shape\"]\n",
    "        images[i,:h,:w] = row.image\n",
    "        sizes[i,:] = h,w\n",
    "        challenges[i] = challenge_index.get_loc(row.challenge)\n",
    "        itype[i] = itype_index.get_loc(row.type)\n",
    "    data = xa.Dataset(\n",
    "        dict(\n",
    "            images = ((\"idx\",\"row\",\"col\"), images),\n",
    "            sizes = ((\"idx\",\"dim\"), sizes),\n",
    "            challenges = ((\"idx\",),challenges),\n",
    "            itype = ((\"idx\",),itype),\n",
    "        ),\n",
    "        coords = dict(\n",
    "            dim = pd.Index([\"row\",\"col\"]),\n",
    "        ),\n",
    "    )\n",
    "    padded_data[gs] = data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c05cc-d8ec-4bdb-a8de-3d2333b0eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(batch_size: int, rngs:nnx.Rngs):\n",
    "    weights = {}\n",
    "    for gs,data in padded_data.items():\n",
    "        weights[gs] = data.idx.size\n",
    "    logits = np.log(np.array(list(weights.values())))\n",
    "    keys = tuple(weights.keys())\n",
    "    rng = rngs\n",
    "    seen = set()\n",
    "    while True:\n",
    "        gs = keys[rng.categorical(logits)]\n",
    "        is_first_of_kind = gs not in seen\n",
    "        seen.add(gs)\n",
    "        data = padded_data[gs]\n",
    "        assert data.idx.size >= batch_size\n",
    "        i = set()\n",
    "        while len(i) < batch_size:\n",
    "            j = rng.randint((batch_size-len(i),),0,data.idx.size)\n",
    "            i.update(int(v) for v in j)\n",
    "        i = np.array(sorted(i))\n",
    "        images = data.images.to_numpy()[i]\n",
    "        sizes = data[\"sizes\"].to_numpy()[i]\n",
    "        labels = data.challenges.to_numpy()[i]\n",
    "        yield dict(\n",
    "            inputs = dict(image=images, size=sizes),\n",
    "            label = labels,\n",
    "        ), is_first_of_kind, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beebdf2-e4ab-4908-943a-21cc6826bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2820c741-633e-4a27-b768-8aca53432dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ModelConfig:\n",
    "    num_classes: int = 1000\n",
    "    embed_dim: int = 256\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainConfig:\n",
    "    \"\"\"Configuration for the training script.\"\"\"\n",
    "    seed: int = 42\n",
    "    global_batch_size: int = 128  # across all devices\n",
    "    ref_batch: int = 1024 # all learning rates refer to this batch size\n",
    "    ref_epoch: int = 800000 # number of images that we call one epoch in reporting\n",
    "    # Optimiser\n",
    "    learning_rate: float = 3e-4\n",
    "    betas: tuple[float, float] = (0.9, 0.999)\n",
    "    eps: float = 1e-8\n",
    "    weight_decay: float = 0.05\n",
    "    grad_clip_norm: float = 1.0\n",
    "    # Schedule    \n",
    "    # in units of ref_batch images!\n",
    "    num_train_steps: int = 1000\n",
    "    warmup_steps: float = 10\n",
    "    log_every_steps: int = 50\n",
    "\n",
    "    # TODO: EMA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9fef38-c371-43f1-a1c5-0b8850239c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrainState(nnx.Module):\n",
    "    \"\"\"A frozen dataclass to hold the training state.\"\"\"\n",
    "    config: TrainConfig = dataclasses.field(metadata=dict(static=True))\n",
    "    model: nnx.Module\n",
    "    optimizer: nnx.Optimizer\n",
    "    # ema_params: nnx.Params\n",
    "\n",
    "    @classmethod\n",
    "    def make(\n",
    "        cls,\n",
    "        model: nnx.Module,\n",
    "        config: TrainConfig,\n",
    "        *,\n",
    "        rngs: nnx.Rngs,\n",
    "    ) -> typing.Self:\n",
    "        \"\"\"Initializes the model, optimizer, and the combined training state.\"\"\"\n",
    "\n",
    "        step_scale = config.ref_batch / config.global_batch_size\n",
    "        epoch_scale = config.ref_epoch / config.global_batch_size\n",
    "        # Create the learning rate schedule (warmup + cosine decay is standard for ViTs)\n",
    "        # with Linear LR scaling\n",
    "        lr = config.learning_rate / step_scale\n",
    "        zero_lr = lr * 0.001\n",
    "        lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "            init_value=zero_lr,\n",
    "            peak_value=config.learning_rate,\n",
    "            warmup_steps=int(round(config.warmup_steps*step_scale)),\n",
    "            decay_steps=int(round((config.num_train_steps - config.warmup_steps)*step_scale)),\n",
    "            end_value=zero_lr,\n",
    "        )\n",
    "    \n",
    "        # Create the AdamW optimizer with gradient clipping\n",
    "        tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config.grad_clip_norm),\n",
    "            optax.adamw(\n",
    "                learning_rate=lr_schedule,\n",
    "                weight_decay=config.weight_decay,\n",
    "                b1=config.betas[0],\n",
    "                b2=config.betas[1], \n",
    "                eps=config.eps,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self = cls()\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.optimizer = nnx.Optimizer(model, tx, wrt=nnx.Param)\n",
    "        self.stats = {k:nnx.Variable(0) for k in [\"steps\",\"examples\"]}\n",
    "\n",
    "        return self\n",
    "\n",
    "    def model_stats(self):\n",
    "        stats = {}\n",
    "        for k in [\"model\",\"optimizer\"]:\n",
    "            _, v = nnx.split(getattr(self, k))\n",
    "            leaves = jax.tree_util.tree_leaves(v)\n",
    "            total_params = sum((leaf.size for leaf in leaves), start=0)\n",
    "            total_bytes = sum((leaf.nbytes for leaf in leaves), start=0)\n",
    "            stats[k] = SimpleNamespace(params=total_params,bytes=total_bytes)\n",
    "        \n",
    "        return SimpleNamespace(**stats)\n",
    "\n",
    "    @classmethod\n",
    "    def loss_fn(cls, model, batch, **kw):\n",
    "        inputs = batch[\"inputs\"]\n",
    "        logits = model(inputs[\"image\"], inputs[\"size\"], **kw)\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "            logits=logits, labels=batch[\"label\"],\n",
    "            #label_smoothing=cfg.label_smoothing,\n",
    "        ).mean()\n",
    "        accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == batch[\"label\"])\n",
    "        return loss, dict(loss=loss, accuracy=accuracy)\n",
    "\n",
    "    def batch_stats(self, batch_size:int|None=None,shape:tuple[int,int] = (30,30)):\n",
    "        graph, state = nnx.split(self)\n",
    "        def inference(state, batch):\n",
    "            state = nnx.merge(graph, state)\n",
    "            model = state.model\n",
    "            model.eval()\n",
    "            inputs = batch[\"inputs\"]\n",
    "            return model(inputs[\"image\"], inputs[\"size\"])\n",
    "        def train(state, batch):\n",
    "            state = nnx.merge(graph, state)\n",
    "            model = state.model\n",
    "            model.train()\n",
    "            def loss_fn(model):\n",
    "                return self.loss_fn(model, batch)\n",
    "            grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "            (_, stats), grads = grad_fn(model)\n",
    "\n",
    "            # Update optimizer state and compute new parameters\n",
    "            state.optimizer.update(state.model, grads)\n",
    "            _, state = nnx.split(state)\n",
    "            return state, stats\n",
    "            \n",
    "        if batch_size is None:\n",
    "            batch_size = self.config.global_batch_size\n",
    "        batch = dict(\n",
    "            inputs = dict(\n",
    "                image = np.zeros((batch_size,)+shape,\"i1\"),\n",
    "                size = np.tile(shape,(batch_size,1)),\n",
    "            ),\n",
    "            # TODO: num classes!\n",
    "            label = np.arange(batch_size,dtype=int)%400,\n",
    "        )\n",
    "        stats = {}\n",
    "        for k,fun in dict(\n",
    "            inference=inference,\n",
    "            train=train,\n",
    "        ).items():\n",
    "            # Analyze the forward pass function\n",
    "            jfun = jax.jit(fun)\n",
    "            tfun = jfun.trace(state, batch)\n",
    "            cfun = tfun.lower()#.compile()\n",
    "            cost = cfun.cost_analysis()\n",
    "            stats[k] = SimpleNamespace(\n",
    "                flops = cost.get(\"flops\"),\n",
    "                bytes_accessed = cost.get(\"bytes accessed\"),\n",
    "                bytes_out = cost.get(\"bytes accessedout\"),\n",
    "            )\n",
    "            \n",
    "        return SimpleNamespace(**stats)\n",
    "\n",
    "    def train_step(self, batch, num_devices, **kw):\n",
    "        @nnx.split_rngs(splits=num_devices)\n",
    "        def wrapper(state, batch):\n",
    "            return state._parallel_train_step(batch, tuple(sorted(kw.items())))\n",
    "\n",
    "        batch = jax.tree.map(lambda x:x.reshape(num_devices,-1,*x.shape[1:]), batch)\n",
    "        stats = wrapper(self, batch)\n",
    "\n",
    "        # TODO: this one probably defeats latency hiding\n",
    "        stats = jax.device_get(jax.tree.map(lambda x: x[0], stats))\n",
    "        self.stats[\"steps\"] += 1\n",
    "        self.stats[\"examples\"] += batch[\"label\"].shape[0]\n",
    "        stats.update({k:v.value for k,v in self.stats.items()})\n",
    "        return stats\n",
    "\n",
    "        \n",
    "    @nnx.pmap(\n",
    "        axis_name=\"data\",\n",
    "        in_axes=(nnx.StateAxes({('data',nnx.RngKey,nnx.RngCount): 0, ...: None}),0,None),\n",
    "        static_broadcasted_argnums=2,\n",
    "    )\n",
    "    def _parallel_train_step(self, batch, kw):\n",
    "        def loss_fn(model):\n",
    "            return self.loss_fn(model, batch, **dict(kw))\n",
    "\n",
    "        grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "        (_, stats), grads = grad_fn(self.model)\n",
    "\n",
    "        grads = jax.lax.pmean(grads, axis_name='data')\n",
    "        stats = jax.lax.pmean(stats, axis_name='data')\n",
    "        \n",
    "        self.optimizer.update(self.model, grads)\n",
    "        return stats        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7de7ffd-0929-4f3e-b4c7-909087fa4e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "def main(model, config: TrainConfig, **kw):\n",
    "    \"\"\"The main entry point for the training script.\"\"\"\n",
    "    # Detect available devices (GPUs/TPUs)\n",
    "    num_devices = jax.local_device_count()\n",
    "    is_multi_device = num_devices > 1\n",
    "\n",
    "    per_device_batch_size = config.global_batch_size // num_devices\n",
    "    config = dataclasses.replace(config, global_batch_size=per_device_batch_size*num_devices)\n",
    "\n",
    "    step_scale = config.ref_batch / config.global_batch_size\n",
    "    epoch_scale = config.ref_epoch / config.global_batch_size\n",
    "    \n",
    "    print(f\"--- ARC ViT Pre-training ---\")\n",
    "    print(f\"Detected {num_devices} devices: {jax.devices()}\")\n",
    "    print(f\"Mode: {'Single-device' if not is_multi_device else 'Multi-device (DDP)'}\")\n",
    "    print(f\"Per-device batch size: {per_device_batch_size}\")\n",
    "    print(f\"Global batch size: {config.global_batch_size}\")\n",
    "    print(\"----------------------------\\n\")\n",
    "\n",
    "    # Setup PRNG key\n",
    "    rngs = nnx.Rngs(config.seed)\n",
    "\n",
    "    # 1. Setup Data Pipeline\n",
    "    # ...\n",
    "\n",
    "    # 2. Initialize Training State\n",
    "    state = TrainState.make(\n",
    "        model,\n",
    "        config,\n",
    "        rngs=rngs,\n",
    "    )\n",
    "    \n",
    "    # 4. Start the training loop\n",
    "    print(\"Starting training...\")\n",
    "    start_time = time.monotonic()\n",
    "    prev_images = jit_imgs = 0\n",
    "    prev_elapsed = jit_time = 0\n",
    "    \n",
    "    # Get an iterator for the dataset\n",
    "    ds_iter = iter(make_dataset(config.global_batch_size, rngs=rngs))\n",
    "\n",
    "    try:\n",
    "        was_warmup = True\n",
    "        stats = {}\n",
    "        for step in (pbar := tqdm.auto.trange(int(round(config.num_train_steps*step_scale)))):\n",
    "            # Fetch the next batch of data\n",
    "            batch,is_first_of_kind,epoch = next(ds_iter)\n",
    "            \n",
    "            # Execute one parallel training step\n",
    "            s = state.train_step(batch, num_devices, **kw)\n",
    "    \n",
    "            elapsed_time = time.monotonic() - start_time\n",
    "            images = (step+1)*config.global_batch_size\n",
    "            if is_first_of_kind:\n",
    "                jit_time += elapsed_time - prev_elapsed\n",
    "                jit_imgs += images - prev_images\n",
    "            prev_elapsed = elapsed_time\n",
    "            prev_images = images\n",
    "            images_per_sec = (images-jit_imgs) / (elapsed_time-jit_time) if elapsed_time>jit_time+1 else images/elapsed_time\n",
    "            info = dict(\n",
    "                #step=step,\n",
    "                epoch = epoch,\n",
    "                refstep = images/config.ref_batch,\n",
    "                refepoch= images/config.ref_epoch,\n",
    "                images = images,\n",
    "                images_per_sec=images_per_sec,\n",
    "                seconds_per_epoch=config.ref_epoch/images_per_sec,\n",
    "                **s\n",
    "            )\n",
    "            for k,v in info.items():\n",
    "                stats.setdefault(k,[]).append(v)\n",
    "            pbar.set_postfix(**{\n",
    "                dict(images_per_sec=\"ips\").get(k,k):f\"{info[k]:.{v}f}\"\n",
    "                for k,v in dict(\n",
    "                    loss=2,\n",
    "                    accuracy=3,\n",
    "                    images_per_sec=2,\n",
    "                    refepoch=2,\n",
    "                    epoch=0,\n",
    "                    refstep=2,\n",
    "                ).items()\n",
    "            })\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    print(\"\\n--- Training Finished ---\")\n",
    "    stats = {k:np.array(v) for k,v in stats.items()}\n",
    "    stats = pd.DataFrame(stats)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712b82e-10cd-4da2-befa-6ce1d0914c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.lib.attrs import AttrsModule\n",
    "\n",
    "class Latents(AttrsModule):\n",
    "    mono: SymDecomp  # shape ... N R? Cm\n",
    "    colour: SymDecomp  # shape ... M F R? Cf\n",
    "\n",
    "    @property\n",
    "    def projections(self):\n",
    "        return dict(mono=self.mono,colour=self.colour)\n",
    "\n",
    "    def map_projections(\n",
    "        self, fun: typing.Callable[[SymDecomp], SymDecomp], *other: Self, **kw\n",
    "    ) -> Self:\n",
    "        return attrs.evolve(\n",
    "            self,\n",
    "            **{\n",
    "                k: fun(v, *[getattr(o, k) for o in other], **kw)\n",
    "                for k, v in self.projections.items()\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def map_representations(\n",
    "        self, fun: typing.Callable[[jt.Float], jt.Float], *other: Self, **kw\n",
    "    ) -> Self:\n",
    "        return attrs.evolve(\n",
    "            self,\n",
    "            **{\n",
    "                k: v.map_representations(fun, *[getattr(o, k) for o in other], **kw)\n",
    "                for k, v in self.projections.items()\n",
    "            },\n",
    "        )\n",
    "\n",
    "class PerceiverXAttnLayer(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        hidden_size: FieldDims,\n",
    "        latent_size: SymDecompDims,\n",
    "        qkv_features: int,\n",
    "        *,\n",
    "        global_mix_reduction: int = 4,\n",
    "        num_groups: int | None = None,\n",
    "        dtype: Dtype | None = None,\n",
    "        param_dtype: Dtype = jnp.float32,\n",
    "        attention_dtype: Dtype | None = None,\n",
    "        # broadcast_dropout: bool = True,\n",
    "        dropout_rate: float = 0.0,\n",
    "        deterministic: bool = False,\n",
    "        precision: PrecisionLike = None,\n",
    "        kernel_init: Initializer = default_kernel_init,\n",
    "        # out_kernel_init: Initializer | None = None,\n",
    "        bias_init: Initializer = default_bias_init,\n",
    "        # out_bias_init: Initializer | None = None,\n",
    "        use_bias: bool = True,\n",
    "        hdrs_attend: bool = False,\n",
    "        # attention_fn: Callable[..., Array] = dot_product_attention,\n",
    "        normalize_qk: bool = False,\n",
    "        keep_rngs: bool = True,\n",
    "        rngs: rnglib.Rngs,        \n",
    "    ):\n",
    "        def make_linear(inf, outf, *, cls=SymmetricLinear):\n",
    "            # TODO: do we have all relevant parameters?\n",
    "            return cls(\n",
    "                inf,\n",
    "                outf,\n",
    "                dtype=dtype,\n",
    "                param_dtype=param_dtype,\n",
    "                kernel_init=kernel_init,\n",
    "                bias_init=bias_init,\n",
    "                use_bias=use_bias,\n",
    "                precision=precision,\n",
    "                rngs=rngs,\n",
    "            )\n",
    "        self.n_features = n_features = qkv_features // num_heads\n",
    "        nkv = 2 * num_groups * n_features\n",
    "        nv = num_groups * n_features\n",
    "        self.kv = hidden_size.map_projections(\n",
    "            lambda k, v: make_linear(\n",
    "                v,\n",
    "                attrs.evolve(v, inv=nkv, equiv=nv),\n",
    "            ),\n",
    "            cls=dict,\n",
    "        )\n",
    "        nq = num_heads * n_features\n",
    "        self.q = latent_size.map_projections(\n",
    "            lambda k, v: make_linear(\n",
    "                v,\n",
    "                attrs.evolve(v, inv=nq, equiv=0),\n",
    "            ),\n",
    "            cls=dict,\n",
    "        )\n",
    "\n",
    "    def __call__(self, hidden: Field, latent: Latents) -> Latents:\n",
    "        assert self.hidden_size.validate(\n",
    "            hidden\n",
    "        ), self.hidden_size.validation_problems(features)\n",
    "        assert hidden.context.rep == hidden.cells.rep\n",
    "\n",
    "        R = hidden.cells.rep.dim\n",
    "        batch = hidden.cells.equiv.shape[:-5]\n",
    "        B = int(np.prod(batch))\n",
    "        Y, X, F = hidden.cells.equiv.shape[-5:-2]\n",
    "        H = self.n_features\n",
    "        D = H\n",
    "        K = self.num_groups\n",
    "        N = self.num_heads\n",
    "\n",
    "        inputs = [\n",
    "            r for p in hidden.projections.values() for r in p.representations.values()\n",
    "        ] + [\n",
    "            r for p in latent.projections.values() for r in p.representations.values()\n",
    "        ]\n",
    "        dtype = nnx.nn.dtypes.canonicalize_dtype(*inputs, dtype=self.dtype)\n",
    "        attention_dtype = nnx.nn.dtypes.canonicalize_dtype(\n",
    "            *inputs, dtype=self.attention_dtype\n",
    "        )\n",
    "        attention_dtype = jnp.promote_types(attention_dtype, jnp.float32)\n",
    "\n",
    "        precision = self.precision\n",
    "\n",
    "        # first; input projections\n",
    "        qkv = {}\n",
    "        for proj,src,tgt in [(self.q, latent, \"Q\"),(self.kv, hidden, \"KV\")]:\n",
    "            for k, v in proj.items():\n",
    "                inp = getattr(src, k)\n",
    "                out = v(inp)\n",
    "                rep = out.rep\n",
    "                equiv = out.equiv\n",
    "                inv = out.inv\n",
    "    \n",
    "                d = qkv.setdefault(k,{})\n",
    "                for kk in tgt:\n",
    "                    n = dict(Q=N * H, K=K * H, V=K * D)[kk]\n",
    "                    if kk in \"QK\":\n",
    "                        maybe_cast = lambda v: v.astype(attention_dtype)\n",
    "                    else:\n",
    "                        # jax.nn.dot_product_attention does not support mixed precision\n",
    "                        maybe_cast = lambda v: v.astype(attention_dtype)\n",
    "                    e = maybe_cast(equiv[..., :n])\n",
    "                    # print(f\"equiv {k}.{kk}.shape = {d[kk].shape}\")\n",
    "                    equiv = equiv[..., n:]\n",
    "                    i = maybe_cast(inv[..., :n])\n",
    "                    # print(f\"inv {k}.{kk}.shape = {di[kk].shape}\")\n",
    "                    inv = inv[..., n:]\n",
    "                    d[kk] = SymDecomp(inv=i,equiv=e,rep=rep)\n",
    "                assert not inv.size, f\"{k}: {inv.shape=}\"\n",
    "                assert not equiv.size\n",
    "        qkv = SimpleNamespace(**{k:SimpleNamespace(**v) for k,v in qkv.items()})\n",
    "\n",
    "        # second; X-attn to row/col headers (only mono)\n",
    "        hdr_attn = []\n",
    "        for axis in range(2):\n",
    "            hdr = [qkv.cols, qkv.rows][axis]\n",
    "            out = jax.nn.dot_product_attention(\n",
    "                query=jnp.swapaxes(qkv.mono.Q.inv[:, :1, :, :, :], -3, -4).reshape(\n",
    "                    B * F, 1, N, 2 * H\n",
    "                ),\n",
    "                key=jnp.swapaxes(efc.K, -3, -4).reshape(B * F, Y * X + 1, K, 2 * H),\n",
    "                value=jnp.swapaxes(efc.V, -3, -4).reshape(B * F, Y * X + 1, K, D),\n",
    "                mask=jnp.concatenate(\n",
    "                    [\n",
    "                        # TODO: should we self-attend here?\n",
    "                        jnp.zeros((B * F, 1, 1, 1), bool),\n",
    "                        jnp.tile(features.mask, (F, 1)).reshape(-1, 1, 1, Y * X),\n",
    "                    ],\n",
    "                    axis=-1,\n",
    "                ),\n",
    "            ).astype(dtype)\n",
    "            hdr_attn.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c36d2-3558-4cec-91b0-2beeec1703a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = FieldDims.make(\n",
    "    inv_fac = 2,\n",
    "    context = 32,\n",
    "    hdrs = 16,\n",
    "    cells = 16,\n",
    ")\n",
    "arc_cls = ARCClassifier(\n",
    "    hidden_size = dims,\n",
    "    mha_features = 24*2,\n",
    "    mlp_width_factor = 2,\n",
    "    num_heads = 2,\n",
    "    num_groups = 1,\n",
    "    num_classes = 400,\n",
    "    num_layers = 8,\n",
    "    rngs = nnx.Rngs(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf7fff-a6e3-4c39-ba76-d99006f0bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainConfig(\n",
    "    global_batch_size = 16,\n",
    "    num_train_steps = 1000,\n",
    "    warmup_steps = 5,\n",
    "    learning_rate = 3e-4,\n",
    "    weight_decay = 0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d1c7bd-c02c-47cd-a987-f9ea1c19a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import humanize\n",
    "\n",
    "if True:\n",
    "    layer = arc_cls.encoder.blocks\n",
    "    gs,state = nnx.split(layer)\n",
    "    state = jax.tree.map(lambda v:v[0],state)\n",
    "    layer = nnx.merge(gs,state)\n",
    "    \n",
    "    x = dims.make_empty(batch=(1,),shape=(30,30),flavours=11)\n",
    "    def test_fun(model,x,**kw):\n",
    "        return model(x,**kw)\n",
    "    def unpack(obj,path=()):\n",
    "        match obj:\n",
    "            case SimpleNamespace():\n",
    "                yield from unpack(vars(obj),path)\n",
    "            case dict():\n",
    "                for k,v in obj.items():\n",
    "                    yield from unpack(v,path+(k,))\n",
    "            case SymmetricLinear():\n",
    "                yield path,obj.approximate_flops\n",
    "            case nnx.Linear():\n",
    "                yield path,obj.kernel.size\n",
    "            case _:\n",
    "                print(f\"Ignoring {type(obj).__name__}\")\n",
    "    attn = layer.attn\n",
    "    tot = 0\n",
    "    for k,v in unpack(dict(mix=attn.cell_global_mix,qkv=attn.qkv,out=attn.out)):\n",
    "        F = 11\n",
    "        N = 30\n",
    "        fac = dict(\n",
    "            mix=F,\n",
    "            context=F,\n",
    "            rows=F*N,\n",
    "            cols=F*N,\n",
    "            cells=F*N*N,\n",
    "        ).get(k[-1])\n",
    "        tot += v*fac\n",
    "        print(f\"{'.'.join(k)}: {humanize.naturalsize(v*fac)}\")\n",
    "    print(f\"Total attention projections: {humanize.naturalsize(tot)}\")\n",
    "    compiled = nnx.jit(test_fun).trace(layer.attn,x,deterministic=True).lower().compile()\n",
    "    print(f\"Total attn: {humanize.naturalsize(compiled.cost_analysis()[\"flops\"])}\")\n",
    "    compiled = nnx.jit(test_fun).trace(layer.mlp,x,rngs=nnx.Rngs(0)).lower().compile()\n",
    "    print(f\"Total MLP: {humanize.naturalsize(compiled.cost_analysis()[\"flops\"])}\")\n",
    "    compiled = nnx.jit(test_fun).trace(layer,x,rngs=nnx.Rngs(0)).lower().compile()\n",
    "    print(f\"Total Layer: {humanize.naturalsize(compiled.cost_analysis()[\"flops\"])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254284da-1dac-4958-8642-041bea326a38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import humanize\n",
    "\n",
    "def print_stats(stats,path=()):\n",
    "    for k,v in vars(stats).items():\n",
    "        p = path+(k,)\n",
    "        if isinstance(v, SimpleNamespace):\n",
    "            print_stats(v, p)\n",
    "            continue\n",
    "        if v is None:\n",
    "            continue\n",
    "        if p[0] == \"model\":\n",
    "            va = {\"\":v}\n",
    "        else:\n",
    "            va = dict(batch=v,example=v/config.global_batch_size)\n",
    "        msg = []\n",
    "        for kk,vv in va.items():\n",
    "            if \"bytes\" in k:\n",
    "                n = humanize.naturalsize(vv,binary=True)\n",
    "            else:\n",
    "                n = humanize.naturalsize(vv,gnu=True,format=\"%.1f \")\n",
    "                if n.endswith(\"B\"):\n",
    "                    n = n[:-1]+\" \"\n",
    "            if \"flops\" in k:\n",
    "                n += \"FLOPs\"\n",
    "            if kk:\n",
    "                n += f\" ({kk})\"\n",
    "            msg.append(n)\n",
    "        n = \"\".join(f\"{n:25s}\" for n in msg)\n",
    "        print(f\"{'.'.join(p):32s}: {n}\")\n",
    "\n",
    "\n",
    "ts = TrainState.make(model=arc_cls, config=config, rngs=nnx.Rngs(0))\n",
    "stats = SimpleNamespace(model=ts.model_stats(),batch=ts.batch_stats())\n",
    "leaves = [\n",
    "    a for p in dims.make_empty(\n",
    "        batch=(config.global_batch_size,),\n",
    "        shape=(30,30),\n",
    "        flavours=11,\n",
    "    ).projections.values() for a in p.representations.values()\n",
    "]\n",
    "stats.fields = SimpleNamespace(params=sum(a.size for a in leaves),bytes=sum(a.nbytes for a in leaves))\n",
    "print_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb2cae-8adb-437a-ae5a-6eed6451f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = main(arc_cls, config, remat=True, unroll=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4fd383-a3ff-46ac-8965-c906a4d0f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "\n",
    "def filt(signal, filter_width = 50):\n",
    "    signal = ndimage.uniform_filter1d(signal, filter_width, mode=\"constant\")\n",
    "    signal /= ndimage.uniform_filter1d(np.ones_like(signal), filter_width, mode=\"constant\")\n",
    "    return signal\n",
    "\n",
    "fig,axes = plt.subplots(2,1,figsize=(8,8))\n",
    "ax = axes[0]\n",
    "ax.semilogy(stats.refstep, filt(stats.loss))\n",
    "ax = axes[1]\n",
    "ax.plot(stats.refstep, filt(stats.accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ac2226-c964-49c9-b435-7bde74ee485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import msgpack\n",
    "import lzma\n",
    "import anyio\n",
    "import pickle\n",
    "\n",
    "asdfasd\n",
    "\n",
    "async with await anyio.open_file(data_root/\"arc-vision-model.msgpack.xz\",\"wb\") as fh:\n",
    "    compressor = lzma.LZMACompressor()\n",
    "    \n",
    "    async def write(*data):\n",
    "        serialised = msgpack.dumps(tuple(data))\n",
    "        compressed = await anyio.to_thread.run_sync(compressor.compress,serialised)\n",
    "        await fh.write(compressed)\n",
    "    \n",
    "    graphdef,flatstate = nnx.graph.flatten(nnx.pure(nnx.state(arc_cls, nnx.Param)))\n",
    "    await write(\"G\",pickle.dumps(graphdef))\n",
    "    pointer = ()\n",
    "    for path,s in tqdm.auto.tqdm(flatstate):\n",
    "        pstr = \".\".join(str(v) for v in path)\n",
    "        assert isinstance(s, jaxlib._jax.ArrayImpl)\n",
    "        d = np.asarray(jax.device_get(s))\n",
    "        neq = 0\n",
    "        for i,(a,b) in enumerate(zip(pointer, path)):\n",
    "            neq = i\n",
    "            if a != b:\n",
    "                break\n",
    "        key = (neq,path[neq:])\n",
    "        await write(\"A\",neq,path[neq:],d.shape,str(d.dtype),bytes(d.data))\n",
    "        pointer = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a405c6-492d-44cd-8b07-68638528ad0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
