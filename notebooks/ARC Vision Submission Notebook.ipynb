{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0643eed6-ec73-45b5-9efc-5259ba7afb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import lzma\n",
    "import os\n",
    "import dataclasses\n",
    "import itertools\n",
    "import functools\n",
    "import json\n",
    "import contextlib\n",
    "import zipfile\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "from types import MappingProxyType, SimpleNamespace\n",
    "\n",
    "import cbor2\n",
    "import attrs\n",
    "import tqdm.auto\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import etils.epath\n",
    "from flax import nnx\n",
    "import optax\n",
    "import jaxtyping as jt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from arc25 import symmetry, tools as arc25_tools\n",
    "from arc25.symmetry import D4, transform_vector\n",
    "from arc25 import serialisation\n",
    "from arc25.dsl.types import Vector, Dir4\n",
    "from arc25.vision2.symrep import SymDecompBase, SplitSymDecomp, SymDecompDims, standard_rep, RepSpec\n",
    "from arc25.vision2.fields import FieldDims, CoordinateGrid\n",
    "from arc25.vision2.linear import SpaceSymmetricLinear, SpaceSymmetricTensor, SymmetryMappingSpec, SymDecompLinear\n",
    "from arc25.vision2 import fields, attention, encoder, transformer, mae, swiglu, arc_solver\n",
    "from arc25.training import saving, dataset, mae as mae_trainer, knn_eval, linear_probe, arc_solver as solver_trainer, row_weighted_adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58db4ed7-fc15-48a1-998a-efe35e40b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"XLA_FLAGS\"]=\"--xla_force_host_platform_device_count=2\"\n",
    "os.environ[\"EPATH_USE_TF\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17eb3ed3-51ca-4338-9d3f-63b7fa30972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_root = Path(\"..\").resolve()\n",
    "data_root = proj_root / \"data\"\n",
    "model_dir = data_root / \"models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f39ec8-3813-4544-afd3-2f4e05651a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(data_root/\"external/arc-prize-2025.zip\",\"r\") as zfh:\n",
    "    with zfh.open(\"arc-agi_test_challenges.json\",\"r\") as fh:\n",
    "        raw_challenge_data = json.load(fh)\n",
    "    with zfh.open(\"sample_submission.json\",\"r\") as fh:\n",
    "        sample_submission = json.load(fh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbee9d23-d9a6-4e32-84c1-c3931924ee82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 30])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_size = np.r_[0, 0]\n",
    "datasets = dict(train=[], test=[])\n",
    "for k, v in raw_challenge_data.items():\n",
    "    for typ,dst in datasets.items():\n",
    "        if typ==\"train\" and any(np.array(iop[\"input\"]).shape != np.array(iop[\"output\"]).shape for iop in v[typ]):\n",
    "            continue\n",
    "        for i, iop in enumerate(v[typ]):\n",
    "            for kk in [\"input\",\"output\"]:\n",
    "                if typ==\"test\" and kk==\"output\":\n",
    "                    continue\n",
    "                img = dataset.Image(np.array(iop[kk],\"i1\"))\n",
    "                ex = dataset.ImageExample(\n",
    "                    challenge=k,\n",
    "                    example_idx=i,\n",
    "                    example_type=kk,\n",
    "                    image=img,\n",
    "                )\n",
    "                sh = np.array(img.shape)\n",
    "                max_size = np.maximum(max_size, sh)\n",
    "                dst.append(ex)\n",
    "challenges = frozenset(raw_challenge_data)\n",
    "challenge_order = tuple(sorted(challenges))\n",
    "datasets = SimpleNamespace(**{k:dataset.ImagesDataset(\n",
    "    examples=tuple(v),\n",
    "    challenges=challenges,\n",
    "    max_size=tuple(int(v) for v in max_size),\n",
    ") for k,v in datasets.items()})\n",
    "max_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeb56d0b-2716-43cf-b2bd-0fc12bffdb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_solution_attempts = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45b8b62d-434d-4944-92ed-fb22101a16ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = arc_solver.ARCSolver(\n",
    "    **arc_solver.configs[\"small\"],\n",
    "    num_latent_programs = len(challenge_order)*num_solution_attempts,\n",
    "    dtype=jnp.float32,\n",
    "    rngs=nnx.Rngs(42),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d7e41f0-d11c-47d0-9022-6768528861c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arc25.training.cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57d0d924-4dfc-42b7-84df-44f080c6e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chkp_path = etils.epath.Path(\n",
    "    \"gs://576e2361-arc-agi-2/checkpoints/20251030-1638-vertex-ai-arc-solver-small-4xv6e/\"\n",
    "    \"20251030-1638-vertex-ai-arc-solver-small-4xv6e-chkp-000256.msgpack.xz\"\n",
    ")\n",
    "checkpoint_data = saving.load_model(chkp_path)\n",
    "solver_checkpoint = checkpoint_data.state.model\n",
    "del solver_checkpoint[\"latent_program_embeddings\"]\n",
    "nnx.update(solver, solver_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e6cf04d-3a3c-40c5-8a65-7cc8c838a1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_devices = jax.local_device_count()\n",
    "\n",
    "bucket_cuts = [12,20,30]\n",
    "bucket_shapes = tuple(sorted(set(\n",
    "   itertools.product(bucket_cuts,bucket_cuts) \n",
    ")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25d0f0da-26f5-4da7-ac78-3ee2918676fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = solver_trainer.ArcSolverConfig(\n",
    "    max_num_epochs = None,\n",
    "    max_num_ref_batches = 1,\n",
    "    eval_batch_size=64,\n",
    ")\n",
    "\n",
    "trainer = solver_trainer.ArcSolverTrainer.make(\n",
    "    config=config,\n",
    "    model=solver,\n",
    "    collator=None,\n",
    "    inputs_src=SimpleNamespace(challenges=challenge_order, bucket_shapes=bucket_shapes),\n",
    "    num_devices=num_devices,\n",
    "    rngs=None,#nnx.Rngs(config.seed),\n",
    "#    lr_schedule=lr_schedule,\n",
    "    eval_dataset=datasets.train,\n",
    "    with_progress_bars=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e22b1729-90e7-47c8-84d6-1101dffd6a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1f29cbee5d4f96ae300e0df5f5801a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing embed_inputs for shape dict(input_sizes=(1,7,2), inputs=(1,7,12,12)) (kw=dict(mode='flat', remat=True, unroll=None, deterministic=True))\n",
      "Tracing embed_inputs for shape dict(input_sizes=(1,64,2), inputs=(1,64,12,12)) (kw=dict(mode='flat', remat=True, unroll=None, deterministic=True))\n",
      "Tracing embed_inputs for shape dict(input_sizes=(1,39,2), inputs=(1,39,12,20)) (kw=dict(mode='flat', remat=True, unroll=None, deterministic=True))\n",
      "Tracing embed_inputs for shape dict(input_sizes=(1,8,2), inputs=(1,8,12,30)) (kw=dict(mode='flat', remat=True, unroll=None, deterministic=True))\n",
      "Tracing embed_inputs for shape dict(input_sizes=(1,22,2), inputs=(1,22,20,20)) (kw=dict(mode='flat', remat=True, unroll=None, deterministic=True))\n",
      "Tracing embed_inputs for shape dict(input_sizes=(1,64,2), inputs=(1,64,20,20)) (kw=dict(mode='flat', remat=True, unroll=None, deterministic=True))\n",
      "Tracing embed_inputs for shape dict(input_sizes=(1,12,2), inputs=(1,12,20,30)) (kw=dict(mode='flat', remat=True, unroll=None, deterministic=True))\n",
      "Tracing embed_inputs for shape dict(input_sizes=(1,47,2), inputs=(1,47,30,30)) (kw=dict(mode='flat', remat=True, unroll=None, deterministic=True))\n"
     ]
    }
   ],
   "source": [
    "trainer._cache_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4365fc9-37fc-4c6b-8b33-dcfe6d194b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, minibatch):\n",
    "    nsa = num_solution_attempts\n",
    "\n",
    "    mbd = jax.tree.map(\n",
    "        lambda a: jnp.tile(a[:, None, ...], (1, nsa) + (1,) * (a.ndim - 1)),\n",
    "        minibatch,\n",
    "    )\n",
    "\n",
    "    embeddings = mbd[\"embeddings\"]\n",
    "    output_sizes = mbd[\"output_sizes\"]\n",
    "    latent_program_idx = mbd[\"latent_program_idx\"] * nsa\n",
    "    latent_program_idx += np.arange(nsa)[\n",
    "        None, :, *(None,) * (latent_program_idx.ndim - 2)\n",
    "    ]\n",
    "\n",
    "    logits = model.decode(\n",
    "        embeddings,\n",
    "        output_size=output_sizes,\n",
    "        latent_program_idx=latent_program_idx,\n",
    "        mode=\"flat\",\n",
    "        remat=True,\n",
    "        unroll=None,\n",
    "        deterministic=True,\n",
    "    ).astype(jnp.float32)\n",
    "            \n",
    "    outputs = mbd[\"outputs\"]\n",
    "    output_masks = mbd[\"output_masks\"]\n",
    "    cell_weights = mbd[\"cell_weight\"]\n",
    "\n",
    "    # Loss on ALL output cells (not masked like MAE)\n",
    "    cell_crossentropy = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=logits, labels=outputs, axis=-1\n",
    "    )\n",
    "\n",
    "    # Mask to valid output regions and weight by pre-normalized cell weights\n",
    "    pair_crossentropy = jnp.where(\n",
    "        output_masks, cell_crossentropy * cell_weights, 0\n",
    "    ).sum(axis=(-2, -1))\n",
    "    loss = pair_crossentropy.sum()\n",
    "\n",
    "    # Per-cell accuracy\n",
    "    predictions = jnp.argmax(logits, axis=-1)\n",
    "    cell_correct = predictions == outputs\n",
    "    cell_accuracy = (\n",
    "        jnp.where(cell_correct & output_masks, cell_weights, 0)\n",
    "        .astype(jnp.float32)\n",
    "        .sum(axis=(-2, -1))\n",
    "    )\n",
    "\n",
    "    # Per-pair accuracy (all cells in output must be correct)\n",
    "    pair_accuracy = (\n",
    "        (\n",
    "            # Padding doesn't count against accuracy\n",
    "            cell_correct\n",
    "            | ~output_masks\n",
    "        )\n",
    "        .all(axis=(-2, -1))\n",
    "        .astype(jnp.float32)\n",
    "    )\n",
    " \n",
    "\n",
    "    stats = dict(\n",
    "        pair_crossentropy = pair_crossentropy,\n",
    "        cell_accuracy = cell_accuracy,\n",
    "        pair_accuracy = pair_accuracy,        \n",
    "    )\n",
    "\n",
    "    pcs = {k[5:]:v for k,v in stats.items() if k.startswith(\"pair_\")}\n",
    "    per_example_stats = jnp.stack(list(pcs.values()), axis=-1)\n",
    "\n",
    "    K = model.latent_program_embeddings.shape[0]\n",
    "    N = per_example_stats.shape[-1]\n",
    "    per_task = (\n",
    "        jnp.zeros((K, N), dtype=per_example_stats.dtype)\n",
    "        .at[latent_program_idx]\n",
    "        .add(per_example_stats)\n",
    "    ).reshape(-1, nsa, N)\n",
    "\n",
    "    stats = {k: v.sum() for k, v in stats.items()} | dict(\n",
    "        per_task={k: per_task[..., i] for i, k in enumerate(pcs)},\n",
    "    )\n",
    "    \n",
    "    return loss, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3218705-b332-4ca7-8359-f3724927998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filter = nnx.PathContains(\"latent_program_embeddings\")\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, minibatch):\n",
    "    grad_fn = nnx.value_and_grad(\n",
    "        loss_fn,\n",
    "        argnums=nnx.DiffState(0, train_filter),\n",
    "        has_aux=True,\n",
    "    )\n",
    "    (_, stats), grads = grad_fn(model, minibatch)\n",
    "    return grads, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72c1d16e-6143-4f73-97d4-1966fb192834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_grads(trainer):\n",
    "    self = trainer\n",
    "    eval_data = self._eval_data_cache\n",
    "\n",
    "    mesh = jax.make_mesh(\n",
    "        (self.num_devices,),\n",
    "        (\"batch\",),\n",
    "        axis_types=(jax.sharding.AxisType.Auto,),\n",
    "    )\n",
    "\n",
    "    def reshard(a, *args):\n",
    "        return jax.device_put(\n",
    "            a,\n",
    "            jax.NamedSharding(mesh, jax.sharding.PartitionSpec(*args)),\n",
    "        )\n",
    "\n",
    "    model_graph, model_state = nnx.split(self.train_state.model)\n",
    "    model_state = jax.tree.map(lambda a: reshard(a), model_state)\n",
    "    resharded_model = nnx.merge(model_graph, model_state)\n",
    "\n",
    "    res = None\n",
    "    with contextlib.ExitStack() as stack:\n",
    "        stack.enter_context(jax.set_mesh(mesh))\n",
    "        if self.with_progress_bars:\n",
    "            import tqdm.auto\n",
    "\n",
    "            pbar = stack.enter_context(\n",
    "                tqdm.auto.tqdm(total=len(eval_data.minibatches), leave=False)\n",
    "            )\n",
    "        else:\n",
    "            pbar = None\n",
    "\n",
    "        for minibatch in eval_data.minibatches:\n",
    "            step_res = train_step(\n",
    "                resharded_model,\n",
    "                minibatch,\n",
    "            )\n",
    "            if res is None:\n",
    "                res = step_res\n",
    "            else:\n",
    "                res = jax.tree.map(lambda a, b: a + b, res, step_res)\n",
    "\n",
    "            if pbar is not None:\n",
    "                pbar.update()\n",
    "\n",
    "    grads, stats = res\n",
    "\n",
    "    per_task = stats.pop(\"per_task\")\n",
    "    shape = (resharded_model.latent_program_tokens.shape[0]//num_solution_attempts, num_solution_attemtps)\n",
    "    per_task = {\n",
    "        k: np.asarray(v)\n",
    "        / np.maximum(1, eval_data.per_class_total_weight).reshape(*shape)\n",
    "        for k, v in per_task.items()\n",
    "    }\n",
    "    stats = {k: float(v) / max(1, eval_data.total_weight) for k, v in res.items()}\n",
    "    stats.update({\n",
    "        f\"best_{k}\":v.max(1).mean()\n",
    "        for k,v in per_task.items()\n",
    "    })\n",
    "    return grads, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c57965f-a196-431d-bf14-bba32c9471c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62eea37ab46e43d784c6c5bc827135da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tx = optax.adam(1e-2,b1=0.9,b2=0.99)\n",
    "optimizer = nnx.Optimizer(\n",
    "    trainer.train_state.model,\n",
    "    tx,\n",
    "    wrt=train_filter,\n",
    ")\n",
    "\n",
    "for step in (pbar:=tqdm.auto.trange(0)):\n",
    "    grads,stats = accumulate_grads(trainer)\n",
    "    optimizer.update(trainer.train_state.model, grads)\n",
    "    print(stats)\n",
    "    asdfasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a3306-5263-4659-ab6f-a86e02250a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
