{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e92cc294-fc57-4ca6-a250-317eeeba70b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from types import MappingProxyType, SimpleNamespace\n",
    "\n",
    "import attrs\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "import optax\n",
    "import jaxtyping as jt\n",
    "import numpy as np\n",
    "\n",
    "from arc25.symmetry import SymOp, transform_vector\n",
    "from arc25.dsl.types import Vector, Dir4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3e29f5e5-edd1-4f50-9b42-e8430e704979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SymRep(opseq=(<SymOp.e: 0>, <SymOp.x: 1>, <SymOp.y: 2>, <SymOp.i: 3>, <SymOp.t: 4>, <SymOp.l: 5>, <SymOp.r: 6>, <SymOp.d: 7>), op2idx=mappingproxy({<SymOp.e: 0>: 0, <SymOp.x: 1>: 1, <SymOp.y: 2>: 2, <SymOp.i: 3>: 3, <SymOp.t: 4>: 4, <SymOp.l: 5>: 5, <SymOp.r: 6>: 6, <SymOp.d: 7>: 7}))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we could have other symmetries\n",
    "AnySymOp: typing.TypeAlias = SymOp\n",
    "\n",
    "@attrs.frozen\n",
    "class SymRep:\n",
    "    # these are not actually operations of the symmetry!\n",
    "    # these are just labels, and symmetry operations connect these labels\n",
    "    opseq: tuple[AnySymOp,...]\n",
    "    op2idx: typing.Mapping[AnySymOp,int] = attrs.field(default=attrs.Factory(\n",
    "        lambda self:MappingProxyType({v:k for k,v in enumerate(self.opseq)}),\n",
    "        takes_self=True,\n",
    "    ))\n",
    "\n",
    "    @classmethod\n",
    "    def from_seq(cls, opseq: typing.Iterable[AnySymOp]) -> typing.Self:\n",
    "        ret = cls(tuple(opseq))\n",
    "        assert ret.is_valid()\n",
    "        return ret\n",
    "\n",
    "    def is_valid(self):\n",
    "        # ensure inverse map is correct \n",
    "        if self.op2idx != {v:k for k,v in enumerate(self.opseq)}:\n",
    "            return False\n",
    "        # ensure group is closed\n",
    "        operations = set(self.opseq[0].inverse.combine(o) for o in self.opseq)\n",
    "        completion = set(o.inverse for o in operations) | set(\n",
    "            a.combine(b) for a in operations for b in operations\n",
    "        )\n",
    "        return completion == operations\n",
    "    \n",
    "    @property\n",
    "    def dim(self)->int:\n",
    "        return len(self.opseq)\n",
    "\n",
    "standard_rep = SymRep.from_seq(SymOp)\n",
    "standard_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "09e8f65d-6a26-4c1e-b142-680132cd8882",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@attrs.frozen\n",
    "class Embedding:\n",
    "    iso: jt.Float[jt.Array, \"... Ci\"]\n",
    "    full: jt.Float[jt.Array, \"... R Cf\"]\n",
    "    rep: SymRep = standard_rep\n",
    "\n",
    "    @property\n",
    "    def shapes(self):\n",
    "        return SimpleNamespace(\n",
    "            iso=self.iso.shape,\n",
    "            full=self.full.shape,\n",
    "            rep=self.rep.dim,\n",
    "        )\n",
    "\n",
    "@attrs.frozen\n",
    "class EmbeddingDims:\n",
    "    iso: int # isotropic values/trivial representation\n",
    "    full: int # full-dimensional representation\n",
    "    rep: SymRep = standard_rep\n",
    "\n",
    "    @property\n",
    "    def dims(self):\n",
    "        return SimpleNamespace(\n",
    "            iso=self.iso,\n",
    "            full=self.full,\n",
    "            rep=self.rep.dim,\n",
    "        )\n",
    "    \n",
    "    def validate(self, embedding: Embedding) -> bool:\n",
    "        try:\n",
    "            np.broadcast_shapes(\n",
    "                embedding.iso.shape[:-1],\n",
    "                embedding.full.shape[:-2],\n",
    "            )\n",
    "        except ValueError:\n",
    "            return False\n",
    "        return (\n",
    "            self.rep == embedding.rep\n",
    "            and self.iso == embedding.iso.shape[-1]\n",
    "            and (self.rep.dim,self.full) == embedding.full.shape[-2:]\n",
    "        )\n",
    "\n",
    "    def make_empty(self, batch: tuple[int,...]=()) -> Embedding:\n",
    "        ret = Embedding(\n",
    "            iso = np.empty(batch+(self.iso,)),\n",
    "            full = np.empty(batch+(self.rep.dim,self.full)),\n",
    "            rep = self.rep,\n",
    "        )\n",
    "        assert self.validate(ret)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "98939c7d-8e7a-42e3-aa39-e08b9b25ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import lax\n",
    "from flax import nnx\n",
    "from flax.nnx.nn.linear import default_kernel_init, default_bias_init\n",
    "from flax.nnx.nn import dtypes\n",
    "from flax.nnx import rnglib\n",
    "from flax.typing import (\n",
    "  Dtype,\n",
    "  Initializer,\n",
    "  PrecisionLike,\n",
    "  DotGeneralT,\n",
    "  PromoteDtypeFn,\n",
    ")\n",
    "\n",
    "class SymmetricLinear(nnx.Module):\n",
    "    \"\"\"Representation-elemet-wise pointwise operation across symmetry and groups of channels; respecting symmetry.\n",
    "\n",
    "    Weight format is (C,R,C')\n",
    "    The layer computes o(n,h,w,r',c') = b(c') + sum_cr k(c,r^-1.r',c') * i(n,h,w,r,c)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: EmbeddingDims,\n",
    "        out_features: EmbeddingDims,\n",
    "        *,\n",
    "        constraint_mode: typing.Literal[\"gather-then-concat\",\"concat-then-gather\"] = \"concat-then-gather\",\n",
    "        use_bias: bool = True,\n",
    "        dtype: Dtype | None = None,\n",
    "        param_dtype: Dtype = jnp.float32,\n",
    "        precision: PrecisionLike = None,\n",
    "        kernel_init: Initializer = default_kernel_init,\n",
    "        bias_init: Initializer = default_bias_init,\n",
    "        dot_general: DotGeneralT = lax.dot_general,\n",
    "        promote_dtype: PromoteDtypeFn = dtypes.promote_dtype,\n",
    "        preferred_element_type: Dtype | None = None,\n",
    "        rngs: rnglib.Rngs,\n",
    "    ):\n",
    "        R = min(in_features.rep.dim, out_features.rep.dim)\n",
    "        kw = dict(\n",
    "            dtype = dtype,\n",
    "            param_dtype = param_dtype,\n",
    "            precision = precision,\n",
    "            kernel_init = kernel_init,\n",
    "            bias_init = bias_init,\n",
    "            dot_general = dot_general,\n",
    "            promote_dtype = promote_dtype,\n",
    "            # preferred_element_type = preferred_element_type,\n",
    "        )\n",
    "        self.iso2iso = nnx.Linear(\n",
    "            in_features.iso,\n",
    "            out_features.iso,\n",
    "            use_bias = use_bias,\n",
    "            **kw,\n",
    "            rngs = rngs,\n",
    "        ) if in_features.iso and out_features.iso else nnx.data(None)\n",
    "        self.iso2full = nnx.Linear(\n",
    "            in_features.iso,\n",
    "            out_features.full,\n",
    "            use_bias = use_bias,\n",
    "            **kw,\n",
    "            rngs = rngs,\n",
    "        ) if in_features.iso and out_features.full else nnx.data(None)\n",
    "        self.full2iso = nnx.Linear(\n",
    "            in_features.full,\n",
    "            out_features.iso,\n",
    "            use_bias = False,\n",
    "            **kw,\n",
    "            rngs = rngs,\n",
    "        ) if in_features.full and out_features.iso else nnx.data(None)\n",
    "        kernel_key = rngs.params()\n",
    "        self.full2full = nnx.Param(\n",
    "          kernel_init(kernel_key, (in_features.full, R, out_features.full), param_dtype)\n",
    "        )\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.constraint_mode = dict(gtc=\"gather-then-concat\",ctg=\"concat-then-gather\").get(constraint_mode,constraint_mode)\n",
    "        self.use_bias = use_bias\n",
    "        for k,v in kw.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "    def _prepare_kernel(self, kernel):\n",
    "        r\"\"\"\n",
    "        The layer computes o(n,h,w,r',c') = sum_mr k(c,r^-1.r',c') * i(n,h,w,r,c)\n",
    "        This is re-cast into a \"standard\" linear as o'(n;h;w;r',c') = sum_mr i'(n;h;w;r,c)*k'(r,c;r',c')\n",
    "\n",
    "        Thus, we return a kernel of shape (R,C,R',C')\n",
    "\n",
    "        We want the kernel to be symmetry-invariant; that is \\rho(s) Y = \\rho(s) K X = K \\rho(s) X.\n",
    "        Let X = \\sum_v X_v |v>, such that \\rho(s) X = \\sum_v X_v |s.v>\n",
    "        Then Y = \\sum_uv K_uv X_v |u>\n",
    "        Thus \\rho(s) Y = \\sum_uv K_uv X_v |s.u> = \\sum_uvw K_uw X_v |u><w|s.v>\n",
    "        -> <s.u|K|v> = <u|K|s.v> for all u,v,s\n",
    "        We will select an arbitrary channel o, keep <u|K|o> as the kernel element u\n",
    "        and derive the rest from it.\n",
    "        -> <u|K|v> = <o.v^-1.u|K|o> for all u,v\n",
    "        \"\"\"\n",
    "        fi = self.in_features\n",
    "        fo = self.out_features\n",
    "        ri = fi.rep\n",
    "        ro = fo.rep\n",
    "\n",
    "        assert ri == ro, \"Not sure if the representation logic below is correct for representation changes\"\n",
    "        \n",
    "        R = ri.dim\n",
    "        C = fi.full\n",
    "        Rp = ro.dim\n",
    "        Cp = fo.full\n",
    "\n",
    "        mode = self.constraint_mode\n",
    "\n",
    "        # this is that |o>\n",
    "        refop = ri.opseq[0]\n",
    "        \n",
    "        ret = []\n",
    "        for u in ro.opseq:\n",
    "            # apply the representation\n",
    "            ki = np.array([ro.op2idx[refop.combine(v.inverse).combine(u)] for v in ri.opseq],\"i4\")\n",
    "            # reshape into the format we need\n",
    "            if mode == \"gather-then-concat\":\n",
    "                k = kernel[None,:,ki,:]\n",
    "                ret.append(k)\n",
    "            elif mode == \"concat-then-gather\":\n",
    "                ret.append(ki)\n",
    "            else:\n",
    "                raise ValueError(mode)\n",
    "\n",
    "        match mode:\n",
    "            case \"gather-then-concat\":\n",
    "                ret = jnp.concatenate(ret, axis=-4)\n",
    "            case \"concat-then-gather\":\n",
    "                ki = np.array(ret)\n",
    "                k = kernel[:,ki,:]\n",
    "                k = k.transpose(1,0,2,3)\n",
    "                ret = k\n",
    "            case _:\n",
    "                raise KeyError(mode)\n",
    "\n",
    "        return ret\n",
    "\n",
    "    \n",
    "    def __call__(self, inputs: Embedding) -> Embedding:\n",
    "        \"\"\"Applies a linear transformation to the inputs along the last dimension.\n",
    "        \n",
    "        Args:\n",
    "          inputs: The nd-array to be transformed.\n",
    "        \n",
    "        Returns:\n",
    "          The transformed input.\n",
    "        \"\"\"\n",
    "        assert self.in_features.validate(inputs)\n",
    "        \n",
    "        kernel_base = self.full2full.value\n",
    "\n",
    "        xi, xf, kernel_base = self.promote_dtype(\n",
    "          (inputs.iso, inputs.full, kernel_base), dtype=self.dtype\n",
    "        )\n",
    "\n",
    "        # TODO: should the be applied before promotion instead?\n",
    "        kernel = self._prepare_kernel(kernel_base)\n",
    "        \n",
    "        xfa = jnp.mean(xf,axis=-2)\n",
    "\n",
    "        of = self.out_features\n",
    "        yi = [\n",
    "            lin(inp)\n",
    "            for lin,inp in [(self.iso2iso,xi), (self.full2iso,xfa)]\n",
    "            if lin is not None\n",
    "        ]\n",
    "        yi = sum(yi) if yi else jnp.empty(xi.shape[:-1]+(of.iso,),xi.dtype)\n",
    "        \n",
    "        # We use dot_general_kwargs for BC compatibility with\n",
    "        # user custom self.dot_general method which may not have\n",
    "        # preferred_element_type argument to avoid breaking\n",
    "        # existing code\n",
    "        dot_general_kwargs = {}\n",
    "        if False and self.preferred_element_type is not None:\n",
    "            dot_general_kwargs[\"preferred_element_type\"] = self.preferred_element_type\n",
    "        yf = self.dot_general(\n",
    "            xf,\n",
    "            kernel,\n",
    "            (((xf.ndim - 2,xf.ndim - 1), (0,1)), ((), ())),\n",
    "            precision=self.precision,\n",
    "            **dot_general_kwargs,\n",
    "        )\n",
    "        if self.iso2full is not None:\n",
    "            yfa = self.iso2full(xi)\n",
    "            yf = yf + yfa[...,None,:]\n",
    "        ret = Embedding(yi,yf,rep=of.rep)\n",
    "        assert self.out_features.validate(ret)\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b3b3e885-37b1-43d4-b097-49ea76a57908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iso: ok=True\n",
      "full: ok=True\n"
     ]
    }
   ],
   "source": [
    "out = []\n",
    "for m in [\"gtc\",\"ctg\"]:\n",
    "    lin = SymmetricLinear(\n",
    "        EmbeddingDims(3,2),\n",
    "        EmbeddingDims(2,3),\n",
    "        constraint_mode=\"gtc\",\n",
    "        rngs=nnx.Rngs(0),\n",
    "    )\n",
    "    out.append(lin(Embedding(np.arange(3),np.arange(8*2).reshape(8,2))))\n",
    "gtc,ctg = out\n",
    "for k in [\"iso\",\"full\"]:\n",
    "    ok = np.allclose(getattr(gtc,k), getattr(ctg,k))\n",
    "    print(f\"{k}: {ok=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "db7639bc-e8d8-461a-b597-e45e20ff30a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: t l r d\n",
      "t: t l r d | ok\n",
      "l: r d t l | ok\n",
      "r: l t d r | ok\n",
      "d: d r l t | ok\n",
      "x: x y t d\n",
      "x: x y t d | ok\n",
      "y: y x d t | ok\n",
      "t: t d x y | ok\n",
      "d: d t y x | ok\n",
      "e: e i\n",
      "e: e i | ok\n",
      "i: i e | ok\n"
     ]
    }
   ],
   "source": [
    "for sobseq in [\n",
    "    # D2 (never swaps x and y)\n",
    "    standard_rep.opseq[4:],\n",
    "    # parity preserving subset\n",
    "    (SymOp.x, SymOp.y, SymOp.t, SymOp.d),\n",
    "    # C2 \n",
    "    (SymOp.e, SymOp.i),\n",
    "]:\n",
    "    o = sobseq[0]\n",
    "    print(f\"{o.name}: {\" \".join(v.name for v in sobseq)}\")\n",
    "    for u in sobseq:\n",
    "        comb = tuple(o.combine(v.inverse).combine(u) for v in sobseq)\n",
    "        print(f\"{u.name}: {\" \".join(q.name for q in comb)} | {\"ok\" if all(q in sobseq for q in comb) else \"NOK\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "3ea562c9-9108-4852-968e-f0e5792667cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@attrs.frozen\n",
    "class QKV:\n",
    "    query: jt.Float[jt.Array, \"... T P N H 2\"]\n",
    "    key: jt.Float[jt.Array, \"... S P K H 2\"]\n",
    "    value: jt.Float[jt.Array, \"... S P K D\"]\n",
    "    mask: jt.Bool[jt.Array, \"... S\"]| None = None\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        T,P,N,H,two = self.query.shape[-5:]\n",
    "        S = self.key.shape[-4]\n",
    "        K = self.key.shape[-2]\n",
    "        D = self.value.shape[-1]\n",
    "        return SimpleNamespace(\n",
    "            T=T,P=P,N=N,H=H,S=S,K=K,D=D,batch=self.query.shape[:-5],\n",
    "        )\n",
    "    \n",
    "    def validation_problems(self):\n",
    "        T,P,N,H,two = self.query.shape[-5:]\n",
    "        S = self.key.shape[-5]\n",
    "        K = self.key.shape[-3]\n",
    "        D = self.value.shape[-1]\n",
    "        if two != 2:\n",
    "            return \"query 2\"\n",
    "        if self.key.shape[-5:] != (S,P,K,H,2):\n",
    "            return \"key\"\n",
    "        if self.value.shape[-4:] != (S,P,K,D):\n",
    "            return \"value\"\n",
    "        if self.mask.shape[-1] != S:\n",
    "            return \"mask\"\n",
    "        try:\n",
    "            np.broadcast_shapes(\n",
    "                 self.query.shape[:-5],\n",
    "                 self.key.shape[:-5],\n",
    "                 self.value.shape[:-4],\n",
    "                 self.mask.shape[:-1],\n",
    "            )\n",
    "        except ValueError:\n",
    "            return \"batch\"\n",
    "\n",
    "    def is_valid(self):\n",
    "        return not self.validation_problems()\n",
    "\n",
    "def attention_RoPE_with_global(\n",
    "    globl: QKV,\n",
    "    axial: QKV,\n",
    "    pQ: jt.Float[jt.Array, \"... T K H\"],\n",
    "    pK: jt.Float[jt.Array, \"... S K H\"] | None=None,\n",
    "    *,\n",
    "    # this one is usually static; values are 0: normal, 1: reverse\n",
    "    polarisation: jt.Int[jt.Array, \"P\"],\n",
    "):\n",
    "    print(f\"{globl.shape=} {axial.shape=} {pQ.shape=}\")\n",
    "    assert globl.is_valid(), f\"{globl.validation_problems()}: q={globl.query.shape} k={globl.key.shape} v={globl.value.shape}\"\n",
    "    assert axial.is_valid(), axial.validation_problems()\n",
    "\n",
    "    # global and axial need to be mostly consistent\n",
    "    sa = axial.shape\n",
    "    sg = globl.shape\n",
    "    for k,v in vars(sa).items():\n",
    "        if k in {\"S\",\"T\"}:\n",
    "            continue\n",
    "        assert getattr(sg,k) == v, f\"{k}: {getattr(globl,k)} <> {v}\"\n",
    "    assert sa.T == sa.S, f\"RoPE requires equal S & T: {sa}\"\n",
    "    assert pQ.shape == sa.batch+(sa.T,sa.K,sa.H), f\"{pQ.shape=} {sa}\"\n",
    "    assert pK is None or pK.shape == sa.batch+(sa.T,sa.K,sa.H), f\"{pK.shape=} {sa}\"\n",
    "    \n",
    "    # calculate rotation matrices; these have shape [... S/T P K H 2 2]: (length, polarisation, head, feature, u, v)\n",
    "    phi = []\n",
    "    for p in [pQ,pK]:\n",
    "        if p is None:\n",
    "            phi.append(phi[-1])\n",
    "            continue\n",
    "        cs,sn = jnp.cos(p), jnp.sin(p)\n",
    "        nsn = -sn\n",
    "        # rd will have shape ... S/T K H 3\n",
    "        rd = jnp.moveaxis(jnp.array([cs,sn,nsn]),0,-1)\n",
    "        # idx will have shape 2 2 2\n",
    "        idx = np.r_[0,2,1,0,0,1,2,0].reshape(2,2,2)\n",
    "        # r will have shape ... S/T 2 K H 2 2\n",
    "        r = jnp.moveaxis(rd[...,idx],-3,-5)\n",
    "        print(f\"{p.shape=} {rd.shape=} {r.shape=}\")\n",
    "        # r now will have the final target shape\n",
    "        r = r[...,polarisation,:,:,:,:]\n",
    "        print(f\"{polarisation.shape=} {r.shape=}\")\n",
    "        phi.append(r)\n",
    "    rQ,rK = phi\n",
    "    if pK is None:\n",
    "        pK = pQ\n",
    "    \n",
    "    Sa,K,H = axial.key.shape[-4:-1]\n",
    "    Sg,_,D = globl.value.shape[-3:]\n",
    "    Na = axial.query.shape[-2]\n",
    "    Tg,Ng = globl.query.shape[-3:-1]\n",
    "    assert not Na % K\n",
    "    assert not Ng % K\n",
    "    Ma = Na // K\n",
    "    Mg = Ng // K\n",
    "    \n",
    "    aQ = jnp.einsum(\"...tpmkhu, tpkuv -> ...tpmkv\", axis.query.reshape(*axis.query.shape[:-4],L,K,Ma,H,2), rQ)\n",
    "    aK = jnp.einsum(\"...spkhu, spkuv -> ...spkv\", axis.key, rK)\n",
    "    aV = axis.value\n",
    "\n",
    "    gQ = globl.query.reshape(*globl.query.shape[:-4],Tg,K,Mg,H,2)\n",
    "    gK = globl.key\n",
    "    gV = globl.value\n",
    "\n",
    "    log_aa = jnp.einsum(\"...tpkmhv,...spkhv -> ...pkmts\", aQ, aK)\n",
    "    log_gg = jnp.einsum(\"...tpkmhv,...spkhv -> ...pkmts\", gQ, gK)\n",
    "    log_ga = jnp.einsum(\"...tpkmhv,...spkhv -> ...pkmts\", gQ, aK)\n",
    "    log_ag = jnp.einsum(\"...tpkmhv,...spkhv -> ...pkmts\", aQ, gK)\n",
    "    \n",
    "    scale = 1/np.sqrt(H)\n",
    "    V = jnp.concatenate([gV, aV],axis=-3)\n",
    "    msh = np.broadcast_shapes(*[arg.mask.shape[:-1] for arg in [globl, axial] if arg.mask is not None])\n",
    "    msk = jnp.concatenate([\n",
    "        jnp.ones(msh+arg.value.shape[-3:-2]) if arg.mask is None else arg.mask\n",
    "        for arg in [globl,axial]\n",
    "    ]) if msh else None\n",
    "\n",
    "    if Ma == Mg:\n",
    "        assert K*Ma == Na == Ng == K*Mg\n",
    "        N = Na\n",
    "        logits = jnp.block([[log_gg,log_ga],[log_ag,log_aa]])\n",
    "        P = jax.nn.softmax(logits*scale, axis=-1, where=msk)\n",
    "        result = jnp.einsum(\"...pkmts,...spkd -> ...tpkmd\",P,V)\n",
    "        result = result.reshape(*result.shape[:-3],-1,D)\n",
    "        assert result.shape[-3:-1] == (Tg+Ta, N)\n",
    "        globl = result[...,:Tg,:,:,:]\n",
    "        axial = result[...,Tg:,:,:,:]\n",
    "    else:\n",
    "        res = []\n",
    "        for logits in [[log_gg,log_ga], [log_ag,log_aa]]:\n",
    "            logits = jnp.concatenate(logits, axis=-1)\n",
    "            P = jax.nn.softmax(logits*scale, axis=-1, where=msk)\n",
    "            result = jnp.einsum(\"...pkmts,...spkd -> ...tpkmd\",P,V)\n",
    "            result = result.reshape(*result.shape[:-3],-1,D)\n",
    "            res.append(result)\n",
    "        globl, axial = res\n",
    "    return globl, axial\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "feac4938-0ca7-4076-a87d-a78b5311b24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@attrs.frozen\n",
    "class Features:\n",
    "    globl: Embedding # dimensions (... R? C); full representation\n",
    "    rows: Embedding # dimensions (... Y R? C); representation (t,l,r,d)\n",
    "    cols: Embedding # dimensions (... X R? C); representation (e,x,y,i)\n",
    "    cells: Embedding # dimensions (... Y X R? C); full representation\n",
    "    ypos: jt.Float[jt.Array,\"... Y 2\"] # (absolute positions, relative positions)\n",
    "    xpos: jt.Float[jt.Array,\"... X 2\"] # (absolute positions, relative positions)\n",
    "    rmsk: jt.Bool[jt.Array,\"... Y\"]\n",
    "    cmsk: jt.Bool[jt.Array,\"... X\"]\n",
    "    mask: jt.Bool[jt.Array,\"... Y X\"]\n",
    "\n",
    "    @property\n",
    "    def shapes(self):\n",
    "        return SimpleNamespace({\n",
    "            k:v.shapes if isinstance(v, Embedding) else v.shape\n",
    "            for k,v in attrs.asdict(self, recurse=False).items()\n",
    "        })\n",
    "\n",
    "@attrs.frozen\n",
    "class FeatureDim:\n",
    "    globl: EmbeddingDims\n",
    "    rows: EmbeddingDims\n",
    "    cols: EmbeddingDims\n",
    "    cells: EmbeddingDims\n",
    "    # shape is in fact optional, as we don't need it for any weight calculation\n",
    "    shape: tuple[int,int] | None = None\n",
    "\n",
    "    def validity_problem(self):\n",
    "        if not (\n",
    "            self.globl.rep.is_valid()\n",
    "            and self.rows.rep.is_valid()\n",
    "            and self.cols.rep.is_valid()\n",
    "            and self.cells.rep.is_valid()\n",
    "        ):\n",
    "            return \"invalid rep\"\n",
    "        if not set(self.globl.rep.opseq) == set(self.rows.rep.opseq) | set(self.cols.rep.opseq) == set(self.cells.rep.opseq):\n",
    "            return \"rep mismatch\"\n",
    "        if set(self.rows.rep.opseq) & set(self.cols.rep.opseq):\n",
    "            return \"rep overlap\"\n",
    "\n",
    "    def is_valid(self):\n",
    "        return not self.validity_problem()\n",
    "\n",
    "    def validation_problem(self, f: Features):\n",
    "        ret = self.validity_problem()\n",
    "        if ret:\n",
    "            return ret\n",
    "        if not self.globl.validate(f.globl):\n",
    "            return f\"globl {self.globl.dims} != {f.globl.shapes}\"\n",
    "        if not self.rows.validate(f.rows):\n",
    "            return f\"rows {self.rows.dims} != {f.rows.shapes}\"\n",
    "        if not self.cols.validate(f.cols):\n",
    "            return f\"cols {self.cols.dims} != {f.cols.shapes}\"\n",
    "        if not self.cells.validate(f.cells):\n",
    "            return f\"cells {self.cells.dims} != {f.cells.shapes}\"\n",
    "        if self.shape is None:\n",
    "            Y,X = f.cells.full.shape[-4:-2]\n",
    "        else:\n",
    "            Y,X = self.shape\n",
    "        if f.rows.full.shape[-3] != Y:\n",
    "            return f\"rows [{X},{Y}] <> {f.rows.shapes}\"\n",
    "        if f.cols.full.shape[-3] != X:\n",
    "            return f\"cols [{X},{Y}] <> {f.cols.shapes}\"\n",
    "        if f.cells.full.shape[-4:-2] != (Y,X):\n",
    "            return f\"cols [{X},{Y}] <> {f.cells.shapes}\"\n",
    "        if f.ypos.shape[-2:] != (Y,2):\n",
    "            return f\"ypos [{X},{Y}] <> {f.ypos.shape}\"\n",
    "        if f.xpos.shape[-2:] != (X,2):\n",
    "            return f\"xpos [{X},{Y}] <> {f.xpos.shape}\"\n",
    "        if f.rmsk.shape[-1] != Y:\n",
    "            return f\"rmsk [{X},{Y}] <> {f.rmsk.shape}\"\n",
    "        if f.cmsk.shape[-1] != X:\n",
    "            return f\"cmsk [{X},{Y}] <> {f.cmsk.shape}\"\n",
    "        if f.mask.shape[-2:] != (Y,X):\n",
    "            return f\"mask [{X},{Y}] <> {f.mask.shape}\"\n",
    "        try:\n",
    "            np.broadcast_shapes(\n",
    "                f.globl.full.shape[:-2],\n",
    "                f.rows.full.shape[:-3],\n",
    "                f.cols.full.shape[:-3],\n",
    "                f.cells.full.shape[:-4],\n",
    "                f.ypos.shape[:-2],\n",
    "                f.xpos.shape[:-2],\n",
    "                f.rmsk.shape[:-1],\n",
    "                f.cmsk.shape[:-1],\n",
    "                f.mask.shape[:-2],\n",
    "            )\n",
    "            \n",
    "        except ValueError:\n",
    "            return f\"batch {f.shapes}\"\n",
    "\n",
    "    def validate(self, f:Features):\n",
    "        return not self.validation_problem(f)\n",
    "\n",
    "    def make_empty(self, batch:tuple[int,...] = (), *, shape:tuple[int,int] | None = None) -> Features:\n",
    "        if shape is None:\n",
    "            shape = self.shape\n",
    "            assert shape is not None\n",
    "        else:\n",
    "            assert self.shape is None or shape == self.shape\n",
    "        Y,X = shape\n",
    "        ret = Features(\n",
    "            globl = self.globl.make_empty(batch),\n",
    "            rows = self.rows.make_empty(batch+(Y,)),\n",
    "            cols = self.cols.make_empty(batch+(X,)),\n",
    "            cells = self.cells.make_empty(batch+shape),\n",
    "            ypos = np.empty(batch+(Y,2)),\n",
    "            xpos = np.empty(batch+(X,2)),\n",
    "            rmsk = np.empty(batch+(Y,),bool),\n",
    "            cmsk = np.empty(batch+(X,),bool),\n",
    "            mask = np.empty(batch+(Y,X),bool),\n",
    "        ) \n",
    "        assert self.validate(ret), self.validation_problem(ret)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afde70f-0282-487e-be5c-2f74fb42b6e3",
   "metadata": {},
   "source": [
    "## Symmetry-preserving axial attention\n",
    "Applying any point-symmetry to the input field and representation\n",
    "should be equivalent to applying the point symmetry to the output field and representation\n",
    "\n",
    "Formally, axial attention computes:\n",
    "$$\n",
    "Y(t) = \\sum_r |r\\rangle \\int_\\delta p_r[t,\\delta] \\langle r| V(t+\\delta),\n",
    "\\quad\\text{where}\\quad\n",
    "p_r[\\delta] = w_r[\\delta] \\cdot \\mathrm{softmax}_\\delta \\left[ \\langle r| Q(t)) \\cdot \\phi_r(\\delta) \\langle r| K(t+\\delta) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6be0f-e92a-42de-a62d-efa7360b552a",
   "metadata": {},
   "source": [
    "Furthermore, applying the operation $s$ to an input field $X(p)$ has the following effect:\n",
    "$$\n",
    "X'(p) = R_s X(R_s^{-1} p) = \\sum_{uv} |u\\rangle \\langle u| R_s |v\\rangle \\langle v| X(R_s^{-1} p)\n",
    "= \\sum_v |s \\cdot v\\rangle \\langle v| X(R_s^{-1} p),\n",
    "$$\n",
    "where the last equality holds for representation labels matching symmetry operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbd9ce9-a9a2-424f-9f27-df7478924387",
   "metadata": {},
   "source": [
    "Thus, we require the following to hold:\n",
    "$$\n",
    "\\begin{align}\n",
    "Y'(t) &= \\sum_r |r\\rangle \\int_\\delta p'_r[t,\\delta] \\langle r| V'(t+\\delta) \\\\\n",
    "&= \\sum_r |r\\rangle \\int_\\delta p'_r[t,\\delta] \\langle s^{-1} r| V(R_s^{-1} (t+\\delta)) \\\\\n",
    "&= \\sum_{r'} |s\\cdot r'\\rangle \\int_\\delta p'_{s\\cdot r'}[t,\\delta] \\langle r'| V(R_s^{-1} (t+\\delta)) \\\\\n",
    "&= \\sum_{r'} |s\\cdot r'\\rangle \\int_{\\delta'} p'_{s\\cdot r'}[t,R_s \\delta'] \\langle r'| V(R_s^{-1} t+\\delta') \\\\\n",
    "&= R_s \\sum_{r'} |r'\\rangle \\int_{\\delta'} p'_{s\\cdot r'}[t,R_s \\delta'] \\langle r'| V(R_s^{-1} t+\\delta') \\\\\n",
    "&= R_s Y(R_s^{-1} t).\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e673ffb-f276-4d48-9f18-3d996077cf41",
   "metadata": {},
   "source": [
    "A sufficient condition for the last equality is $p'_{s\\cdot r'}[t,R_s \\delta'] = p_{r'}[R_s^{-1} t,\\delta']$ everywhere.\n",
    "Starting from the definition, we have\n",
    "$$\n",
    "\\begin{align}\n",
    "p'_{s\\cdot r'}[t, R_s \\delta'] &= w_{s\\cdot r'}[R_s \\delta'] \\cdot\n",
    "\\mathrm{softmax}_\\delta \\left[ \\langle s\\cdot r'| Q'(t))\n",
    "\\cdot \\phi_{s\\cdot r'}(R_s \\delta') \\langle s\\cdot r'| K'(t+R_s \\delta') \\right] \\\\\n",
    "&= w_{s\\cdot r'}[R_s \\delta'] \\cdot\n",
    "\\mathrm{softmax}_\\delta \\left[ \\langle r'| Q(R_s^{-1} t))\n",
    "\\cdot \\phi_{s\\cdot r'}(R_s \\delta') \\langle r'| K(R_s^{-1} t + \\delta') \\right] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "thus, again, a sufficient condition are $w_{s\\cdot r'}[R_s \\delta'] = w_{r'}[\\delta']$\n",
    "and $\\phi_{s\\cdot r'}(R_s \\delta')=\\phi_{r'}(\\delta')$.\n",
    "This can be implemented by setting $\\phi_r(\\delta) = \\vec \\delta \\cdot R_r \\hat u$, and suitable rotation of $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "59f69925-648d-4bc6-a469-1d06f2db95ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Componet 0=|e>: axial=right transverse=down \n",
      "Componet 1=|x>: axial=left  transverse=down \n",
      "Componet 2=|y>: axial=right transverse=up   \n",
      "Componet 3=|i>: axial=left  transverse=up   \n",
      "Componet 4=|t>: axial=down  transverse=right\n",
      "Componet 5=|l>: axial=up    transverse=right\n",
      "Componet 6=|r>: axial=down  transverse=left \n",
      "Componet 7=|d>: axial=up    transverse=left \n"
     ]
    }
   ],
   "source": [
    "# symmetry-preserving axial attention: \n",
    "# - Applying any point-symmetry to the input field and representation\n",
    "#   should be equivalent to applying the point symmetry to the output field and representation\n",
    "# - Formally; attention computes: $Y(o) = \\sum_s int_d p_u(d) V_u(d)$\n",
    "for i,s in enumerate(standard_rep.opseq):\n",
    "    a = Vector._vec2dir[tuple(transform_vector(s, Vector.RIGHT.as_array()))]\n",
    "    b = Vector._vec2dir[tuple(transform_vector(s, Vector.DOWN.as_array()))]\n",
    "    print(f\"Componet {i}=|{s.name}>: axial={a:5s} transverse={b:5s}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "402dab11-5237-448a-a9ee-c0ce0d8b13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import lax\n",
    "from flax import nnx\n",
    "from flax.nnx.nn.linear import default_kernel_init, default_bias_init, initializers\n",
    "from flax.nnx.nn import dtypes\n",
    "from flax.nnx import rnglib\n",
    "from flax.typing import (\n",
    "  Dtype,\n",
    "  Initializer,\n",
    "  PrecisionLike,\n",
    "  DotGeneralT,\n",
    "  PromoteDtypeFn,\n",
    ")\n",
    "\n",
    "class SymAttention(nnx.Module):\n",
    "    \"\"\"\n",
    "    This module performs axial attention.\n",
    "    \n",
    "    For the \"e\" component of the representation,\n",
    "    we have chosen an arbitrary axis along which to perform the attention;\n",
    "    it determines the axis of attention for all other components.\n",
    "    With trainable frequencies (allowing negative ones),\n",
    "    revesing the direction would be equivalent, but rotations by 90° arent.\n",
    "    Thus, with this choice, we break the symmetry within the\n",
    "    representation. This is fine, if we do this only in one place.\n",
    "    Otherwise, we'd have to augment this with a second attention axis.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        in_features: FeatureDim,\n",
    "        qkv_features: int,\n",
    "        out_features: FeatureDim | None=None,\n",
    "        *,\n",
    "        num_groups: int | None = None,\n",
    "        dtype: Dtype | None = None,\n",
    "        param_dtype: Dtype = jnp.float32,\n",
    "        broadcast_dropout: bool = True,\n",
    "        dropout_rate: float = 0.0,\n",
    "        deterministic: bool | None = None,\n",
    "        precision: PrecisionLike = None,\n",
    "        kernel_init: Initializer = default_kernel_init,\n",
    "        out_kernel_init: Initializer | None = None,\n",
    "        bias_init: Initializer = initializers.zeros_init(),\n",
    "        out_bias_init: Initializer | None = None,\n",
    "        use_bias: bool = True,\n",
    "        # attention_fn: Callable[..., Array] = dot_product_attention,\n",
    "        decode: bool | None = None,\n",
    "        normalize_qk: bool = False,\n",
    "        rngs: rnglib.Rngs,        \n",
    "    ):\n",
    "        if num_groups is None:\n",
    "            num_groups = num_heads\n",
    "\n",
    "        if out_features is None:\n",
    "            out_features = in_features\n",
    "        assert not qkv_features % (2*num_heads)\n",
    "        assert not num_heads % num_groups\n",
    "        self.n_features = n_features = qkv_features // (2*num_heads)\n",
    "        self.in_features = in_features\n",
    "        self.qkv_features = qkv_features\n",
    "        self.out_features = out_features\n",
    "        self.num_heads = num_heads\n",
    "        self.num_groups = num_groups\n",
    "\n",
    "        # frequency is both per group, per features, and linear in both absolute and relative\n",
    "        kernel_key = rngs.params()\n",
    "        freq_init = initializers.normal(1)\n",
    "        self.freqs = nnx.Param(\n",
    "            freq_init(kernel_key, (num_groups, n_features, 2), param_dtype)\n",
    "        )\n",
    "\n",
    "        def make_linear(inf,outf):\n",
    "            # TODO dtypes, biases, etc...\n",
    "            return SymmetricLinear(inf,outf,rngs=rngs)\n",
    "\n",
    "        nqkv = (num_heads+2*num_groups)*n_features*2\n",
    "        nv = num_groups*n_features*2\n",
    "        self.qkv = {\n",
    "            k:make_linear(v,attrs.evolve(v,iso=0,full=nqkv))\n",
    "            for k,v in {k:getattr(in_features,k) for k in [\"globl\",\"rows\",\"cols\",\"cells\"]}.items()\n",
    "        }\n",
    "        self.out = {\n",
    "            k:make_linear(\n",
    "                attrs.evolve(v.out_features,iso=0,full=nv),\n",
    "                getattr(out_features,k),\n",
    "            ) for k,v in self.qkv.items()\n",
    "        }\n",
    "\n",
    "\n",
    "    def __call__(self, features: Features) -> Features:\n",
    "        assert self.in_features.validate(features), self.in_features.validation_problems(features)\n",
    "        assert features.globl.rep == features.cells.rep\n",
    "        \n",
    "        Y,X = features.cells.full.shape[-4:-2]\n",
    "        H = self.n_features\n",
    "        D = 2*H\n",
    "        K = self.num_groups\n",
    "        N = self.num_heads\n",
    "        \n",
    "        xphi = jnp.einsum(\"...xa,kha -> ...xkh\",features.xpos,self.freqs)\n",
    "        yphi = jnp.einsum(\"...ya,kha -> ...ykh\",features.ypos,self.freqs)\n",
    "        phi = [yphi,xphi]\n",
    "        \n",
    "        # first; linear projection into QKV for each of the features separtely\n",
    "        qkv = {}\n",
    "        for k,v in self.qkv.items():\n",
    "            inp = getattr(features, k)\n",
    "            out = v(inp)\n",
    "            assert not out.iso.size\n",
    "            rep = out.rep\n",
    "            out = out.full\n",
    "            d = {}\n",
    "            for kk,n in dict(Q=N*H*2,K=K*H*2,V=K*D).items():\n",
    "                d[kk] = out[...,:n]\n",
    "                out = out[...,n:]\n",
    "            qkv[k] = SimpleNamespace(**d,rep=rep)\n",
    "        qkv = SimpleNamespace(**qkv)\n",
    "        # second; axial attention\n",
    "        gres = []\n",
    "        ares = []\n",
    "        for axis in range(2):\n",
    "            match axis:\n",
    "                case 0:\n",
    "                    maybe_swap = lambda a,i,j: jnp.swapaxes(a, i, j)\n",
    "                case 1:\n",
    "                    maybe_swap = lambda a,i,j: a\n",
    "                case _:\n",
    "                    raise RuntimeError\n",
    "            # careful: performing attention along axis 0, column headers are global, row index acts as position\n",
    "            # so in this case X is a batch dimension, and Y acts as source/target\n",
    "            # globl shape: ... R hd\n",
    "            # hdr shape: ... B R hd\n",
    "            # axial shape\n",
    "            #  - before `maybe_swap`: .... Y X R hd\n",
    "            #  - after  `maybe_swap`: .... B L R hd\n",
    "            hdr = [qkv.cols,qkv.rows][axis]\n",
    "            hmsk = [features.cmsk,features.rmsk][axis]\n",
    "            B = hdr.Q.shape[-3]\n",
    "            Pi = np.array([qkv.cells.rep.op2idx[o] for o in hdr.rep.opseq])\n",
    "            polarisation = np.array([transform_vector(o,Vector.DOWN.as_array())[axis] for o in hdr.rep.opseq])\n",
    "            assert np.all(abs(polarisation) == 1)\n",
    "            polarisation = (polarisation+1)//2\n",
    "            # we concatenate global and axis headers for the KVs\n",
    "            # but there will only be axis headers in the Qs\n",
    "            # concatenation is along S/T, output shape is ... B S/T P hd,\n",
    "            gQ = hdr.Q[...,:,None,:,:]\n",
    "            gK = jnp.concatenate([\n",
    "                jnp.tile(qkv.globl.K[...,None,None,Pi,:],(B,1,1,1)),\n",
    "                hdr.K[...,:,None,:,:],\n",
    "            ],axis=-3)\n",
    "            gV = jnp.concatenate([\n",
    "                jnp.tile(qkv.globl.V[...,None,None,Pi,:],(B,1,1,1)),\n",
    "                hdr.V[...,:,None,:,:],\n",
    "            ],axis=-3)\n",
    "            mask = jnp.tile(hmsk[...,:,None],(1,gK.shape[-3]))\n",
    "\n",
    "            def make_qkv(q,k,v,mask):\n",
    "                # unravel hd -> (N H 2) / (K H 2) / (K D)\n",
    "                return QKV(\n",
    "                    query = q.reshape(*q.shape[:-1],N,H,2),\n",
    "                    key = k.reshape(*k.shape[:-1],K,H,2),\n",
    "                    value = v.reshape(*v.shape[:-1],K,D),\n",
    "                    mask = mask,\n",
    "                )\n",
    "            \n",
    "            res = attention_RoPE_with_global(\n",
    "                globl = make_qkv(\n",
    "                    gQ,gK,gV,\n",
    "                    mask = mask,\n",
    "                ),\n",
    "                axial = make_qkv(\n",
    "                    **{\n",
    "                        k.lower():maybe_swap(v[...,:,:,Pi,:],-4,-3)\n",
    "                        for k,v in vars(qkv.cells).items()\n",
    "                        if k!=\"rep\"\n",
    "                    },\n",
    "                    mask = maybe_swap(features.mask,-2,-1),\n",
    "                ),\n",
    "                pQ = phi[axis],\n",
    "                polarisation = polarisation,\n",
    "            )\n",
    "            ohdr, oax = (v.reshape(*v.shape[:-2],N*D) for v in res)\n",
    "            # ohdr now has dimensions ... B 1 P F\n",
    "            # oax now has dimensions ... B S P F\n",
    "\n",
    "            # TODO: global attention to axis headers\n",
    "            \n",
    "            assert ohdr.shape[-3] == 1\n",
    "            gres.append(ohdr[...,:,0,:,:])\n",
    "            ares.append(maybe_swap(oax,-4,-3))\n",
    "        cells = jnp.concatenate(ares,axis=-2)\n",
    "        orep = SymRep.from_seq(orep)\n",
    "        \n",
    "        # third; global attention\n",
    "        # attention to cells\n",
    "        assert qkv.globl.rep == qkv.cells.rep\n",
    "        globl = jax.nn.dot_product_attention(\n",
    "            # merge R directly into batch dimensions left of it\n",
    "            query = qkv.globl.Q.reshape(-1,1,N,2*H), \n",
    "            # we first need to move R across X and Y before we can merge\n",
    "            key = jnp.moveaxis(qkv.cells.K,-2,-4).reshape(-1,Y*X,K,2*H),\n",
    "            # we first need to move R across X and Y before we can merge\n",
    "            value = jnp.moveaxis(qkv.cells.V,-2,-4).reshape(-1,Y*X,K,D),\n",
    "            mask = jnp.tile(features.mask,(R,1)).reshape(-1,1,1,Y*X),\n",
    "        )\n",
    "        assert globl.shape[-3] == 1\n",
    "        globl = globl[:,0,:,:].reshape(qkv.globl.Q.shape[:-2],R,N*D)\n",
    "        \n",
    "        tmp = dict(\n",
    "            globl = attrs.evolve(features.globl, iso=jnp.empty((0,),dtype), full=globl, rep=qkv.globl.rep),\n",
    "            cols = attrs.evolve(features.cols, iso=jnp.empty((X,0),dtype), full=gres[0], rep=qkv.cols.rep),\n",
    "            rows = attrs.evolve(features.rows, iso=jnp.empty((Y,0),dtype), full=gres[1], rep=qkv.rows.rep),\n",
    "            cells = attrs.evolve(features.cells, iso=jnp.empty((Y,X,0),dtype), full=cells, rep=orep),\n",
    "        )\n",
    "\n",
    "        # finally; output projection\n",
    "        output = attrs.evolve(features,**{k:self.out[k](v) for k,v in tmp.items()})\n",
    "        return output\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "ed8df75e-a001-4261-841e-79029d1cc775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(globl=namespace(iso=(3, 128), full=(3, 8, 64), rep=8),\n",
       "          rows=namespace(iso=(3, 7, 64), full=(3, 7, 4, 32), rep=4),\n",
       "          cols=namespace(iso=(3, 9, 64), full=(3, 9, 4, 32), rep=4),\n",
       "          cells=namespace(iso=(3, 7, 9, 32), full=(3, 7, 9, 8, 16), rep=8),\n",
       "          ypos=(3, 7, 2),\n",
       "          xpos=(3, 9, 2),\n",
       "          rmsk=(3, 7),\n",
       "          cmsk=(3, 9),\n",
       "          mask=(3, 7, 9))"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = FeatureDim(\n",
    "    globl = EmbeddingDims(iso=128,full=64),\n",
    "    rows = EmbeddingDims(iso=64,full=32,rep=SymRep.from_seq((SymOp.t,SymOp.l,SymOp.r,SymOp.d))),\n",
    "    cols = EmbeddingDims(iso=64,full=32,rep=SymRep.from_seq((SymOp.e,SymOp.x,SymOp.y,SymOp.i))),\n",
    "    cells = EmbeddingDims(iso=32,full=16),\n",
    "    shape = (7,9),\n",
    ")\n",
    "assert dim.is_valid()\n",
    "inp = dim.make_empty(batch=(3,))\n",
    "inp.shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "fe6f7317-a027-430e-afb9-691a4fca9031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "globl.shape=namespace(T=1, P=4, N=8, H=8, S=4, K=8, D=16, batch=(3, 9)) axial.shape=namespace(T=7, P=4, N=8, H=8, S=4, K=8, D=16, batch=(3, 9)) pQ.shape=(3, 7, 4, 8)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "RoPE requires equal S & T: namespace(T=7, P=4, N=8, H=8, S=4, K=8, D=16, batch=(3, 9))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[320]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m attn = SymAttention(\u001b[32m8\u001b[39m, dim, \u001b[32m128\u001b[39m, num_groups=\u001b[32m4\u001b[39m, rngs=nnx.Rngs(\u001b[32m0\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m out = \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[318]\u001b[39m\u001b[32m, line 166\u001b[39m, in \u001b[36mSymAttention.__call__\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmake_qkv\u001b[39m(q,k,v,mask):\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# unravel hd -> (N H 2) / (K H 2) / (K D)\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m QKV(\n\u001b[32m    160\u001b[39m         query = q.reshape(*q.shape[:-\u001b[32m1\u001b[39m],N,H,\u001b[32m2\u001b[39m),\n\u001b[32m    161\u001b[39m         key = k.reshape(*k.shape[:-\u001b[32m1\u001b[39m],K,H,\u001b[32m2\u001b[39m),\n\u001b[32m    162\u001b[39m         value = v.reshape(*v.shape[:-\u001b[32m1\u001b[39m],K,D),\n\u001b[32m    163\u001b[39m         mask = mask,\n\u001b[32m    164\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m res = \u001b[43mattention_RoPE_with_global\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mglobl\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_qkv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgK\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxial\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_qkv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43mk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43mmaybe_swap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mPi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcells\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m!=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrep\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaybe_swap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpQ\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi\u001b[49m\u001b[43m[\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolarisation\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolarisation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m ohdr, oax = (v.reshape(*v.shape[:-\u001b[32m2\u001b[39m],N*D) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m res)\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# ohdr now has dimensions ... B 1 P F\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# oax now has dimensions ... B S P F\u001b[39;00m\n\u001b[32m    185\u001b[39m \n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# TODO: global attention to axis headers\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[315]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mattention_RoPE_with_global\u001b[39m\u001b[34m(globl, axial, pQ, pK, polarisation)\u001b[39m\n\u001b[32m     62\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(sg,k) == v, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mgetattr\u001b[39m(globl,k)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m <> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sa.T == sa.S, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRoPE requires equal S & T: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msa\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m pQ.shape == sa.batch+(sa.T,sa.K,sa.H), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpQ.shape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msa\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m pK \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m pK.shape == sa.batch+(sa.T,sa.K,sa.H), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpK.shape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msa\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: RoPE requires equal S & T: namespace(T=7, P=4, N=8, H=8, S=4, K=8, D=16, batch=(3, 9))"
     ]
    }
   ],
   "source": [
    "attn = SymAttention(8, dim, 128, num_groups=4, rngs=nnx.Rngs(0))\n",
    "out = attn(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "571080ac-a764-4889-9a2e-1c5ba3cf7ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'globl': (<SymOp.e: 0>,\n",
       "  <SymOp.x: 1>,\n",
       "  <SymOp.y: 2>,\n",
       "  <SymOp.i: 3>,\n",
       "  <SymOp.t: 4>,\n",
       "  <SymOp.l: 5>,\n",
       "  <SymOp.r: 6>,\n",
       "  <SymOp.d: 7>),\n",
       " 'rows': (<SymOp.t: 4>, <SymOp.l: 5>, <SymOp.r: 6>, <SymOp.d: 7>),\n",
       " 'cols': (<SymOp.e: 0>, <SymOp.x: 1>, <SymOp.y: 2>, <SymOp.i: 3>),\n",
       " 'cells': (<SymOp.e: 0>,\n",
       "  <SymOp.x: 1>,\n",
       "  <SymOp.y: 2>,\n",
       "  <SymOp.i: 3>,\n",
       "  <SymOp.t: 4>,\n",
       "  <SymOp.l: 5>,\n",
       "  <SymOp.r: 6>,\n",
       "  <SymOp.d: 7>)}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:v.in_features.rep.opseq for k,v in attn.qkv.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c9d264-7b95-4743-8635-4841b5f710f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymencLayer(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: FeatureDim,\n",
    "        *,\n",
    "        mlp_width_factor: float,\n",
    "        num_heads: int,\n",
    "        dropout_rate: float = 0.0,\n",
    "        *,\n",
    "        rngs: nnx.Rngs = nnx.Rngs(0),\n",
    "    ) -> None:\n",
    "        def norms(features: FeatureDim = hidden_size,**kw):\n",
    "            return SimpleNamespace({\n",
    "                k:SimpleNamespace({\n",
    "                    kk: nnx.LayerNorm(getattr(getattr(features,k),kk), rngs=rngs, **kw)\n",
    "                    for kk in [\"iso\", \"full\"]\n",
    "                }) for k in [\"globl\", \"rowcol\", \"cells\"]\n",
    "            })\n",
    "        \n",
    "        # First layer normalization using `flax.nnx.LayerNorm`\n",
    "        # before we apply Multi-Head Attentn.\n",
    "        self.norm1 = norms()\n",
    "        # The Multi-Head Attention layer (using `flax.nnx.MultiHeadAttention`).\n",
    "        self.attn = nnx.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            in_features=hidden_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            broadcast_dropout=False,\n",
    "            decode=False,\n",
    "            deterministic=False,\n",
    "            normalize_qk=False, # True to stabilise learning in ViT-22B; see paper http://arxiv.org/abs/2302.05442\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        # Second layer normalization using `flax.nnx.LayerNorm`.\n",
    "        self.norm2 = norms()\n",
    "\n",
    "        # The MLP for point-wise feedforward (using `flax.nnx.Sequential`, `flax.nnx.Linear, flax.nnx.Dropout`)\n",
    "        # with the GeLU activation function (`flax.nnx.gelu`).\n",
    "        self.mlp = nnx.Sequential(\n",
    "            nnx.Linear(hidden_size, mlp_dim, rngs=rngs),\n",
    "            nnx.gelu,\n",
    "            nnx.Dropout(dropout_rate, rngs=rngs),\n",
    "            nnx.Linear(mlp_dim, hidden_size, rngs=rngs),\n",
    "            nnx.Dropout(dropout_rate, rngs=rngs),\n",
    "        )\n",
    "\n",
    "    # The forward pass through the transformer encoder block.\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        # The Multi-Head Attention layer with layer normalization.\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        # The feed-forward network with layer normalization.\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# Example usage for testing:\n",
    "x = jnp.ones((4, 224, 224, 3))\n",
    "model = VisionTransformer(num_classes=1000)\n",
    "y = model(x)\n",
    "print(\"Predictions shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "74b30547-4a8e-4acf-ac2a-a213d6e3f30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028935185185185185"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10e6/4e3/3600/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cce4e0-7364-4b82-ae47-ef27f9df6a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
